{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import optuna\n",
    "\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling\")\n",
    "from preprocessing import to_NCHW, pad_to_384x384, standardize_images\n",
    "from classification import train_and_validate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_amp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data = np.load(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/data/array_data.npz\")\n",
    "unlabeled_images, unlabeled_masks, labeled_images, labeled_masks, labels = data[\"unlabeled_images\"], data[\"unlabeled_masks\"], data[\"labeled_images\"], data[\"labeled_masks\"], data[\"labels\"]\n",
    "\n",
    "unlabeled_images = pad_to_384x384(to_NCHW(unlabeled_images))\n",
    "unlabeled_masks = pad_to_384x384(unlabeled_masks)\n",
    "\n",
    "labeled_images = pad_to_384x384(to_NCHW(labeled_images))\n",
    "labeled_masks = pad_to_384x384(labeled_masks)\n",
    "labels = pad_to_384x384(labels)\n",
    "\n",
    "# Convert to tensors and move to GPU\n",
    "unlabeled_images = torch.tensor(unlabeled_images, dtype=torch.float32).to(device)  # [161, 8, 384, 384]\n",
    "unlabeled_masks = torch.tensor(unlabeled_masks, dtype=torch.bool).to(device)    # [161, 384, 384]\n",
    "\n",
    "labeled_images = torch.tensor(labeled_images, dtype=torch.float32).to(device)      # [3, 8, 384, 384]\n",
    "labeled_masks = torch.tensor(labeled_masks, dtype=torch.bool).to(device)        # [3, 384, 384]\n",
    "labels = torch.tensor(labels, dtype=torch.long).to(device)                      # [3, 384, 384]\n",
    "\n",
    "\n",
    "# Standardize images\n",
    "unlabeled_images, std_channel, mean_channel = standardize_images(unlabeled_images, unlabeled_masks)\n",
    "labeled_images, _, _ = standardize_images(labeled_images, labeled_masks, std_channel, mean_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/jet/home/azhang19/optuna_study_original.pkl\", \"rb\") as f:\n",
    "    study = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0320 01:31:57.903000 36566 site-packages/torch/_logging/_internal.py:1089] [14/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "W0320 01:32:12.274000 36566 site-packages/torch/_dynamo/convert_frame.py:906] [14/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0320 01:32:12.274000 36566 site-packages/torch/_dynamo/convert_frame.py:906] [14/8]    function: 'wrapper' (/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/optimizer.py:473)\n",
      "W0320 01:32:12.274000 36566 site-packages/torch/_dynamo/convert_frame.py:906] [14/8]    last reason: 14/7: Cache line invalidated because L['args'][0] got deallocated\n",
      "W0320 01:32:12.274000 36566 site-packages/torch/_dynamo/convert_frame.py:906] [14/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0320 01:32:12.274000 36566 site-packages/torch/_dynamo/convert_frame.py:906] [14/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.80446373, 0.85723724]), array([0.03354775, 0.02145437]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = labeled_images\n",
    "feature, _, _ = standardize_images(feature, labeled_masks)\n",
    "\n",
    "num_repeat = 100\n",
    "record = np.zeros((num_repeat, 2))\n",
    "classifiers = []\n",
    "\n",
    "for repeat in range(num_repeat):\n",
    "\n",
    "    train_val_idx = [0, 1]\n",
    "\n",
    "    # Container for metrics from each fold\n",
    "    fold_records_f1 = torch.zeros(len(train_val_idx))  # For F1 (objective)\n",
    "    fold_records_acc = torch.zeros(len(train_val_idx))  # For accuracy (logging)\n",
    "\n",
    "    classifiers_in_this_repeat = []\n",
    "    # Assuming feature and labels are defined globally (e.g., torch tensors)\n",
    "    for i in train_val_idx:\n",
    "        # Leave-one-out style split\n",
    "        train_idx = [j for j in train_val_idx if j != i]\n",
    "        val_idx = [i]\n",
    "\n",
    "        # Get training and validation data\n",
    "        train_data = feature[train_idx]\n",
    "        train_labels = labels[train_idx]\n",
    "        val_data = feature[val_idx]\n",
    "        val_labels = labels[val_idx]\n",
    "\n",
    "        # Train and validate, get both F1 and accuracy\n",
    "        classifier, val_f1, val_acc = train_and_validate(\n",
    "            train_data=train_data,\n",
    "            train_labels=train_labels,\n",
    "            val_data=val_data,\n",
    "            val_labels=val_labels,\n",
    "            in_channels=feature.shape[1],\n",
    "            num_layers=study.best_params[\"num_layers\"],\n",
    "            kernel_size=study.best_params[\"kernel_size\"],\n",
    "            hidden_channels=study.best_params[\"hidden_channels\"],\n",
    "            epochs=study.best_params[\"epochs\"],\n",
    "            lr=study.best_params[\"lr\"],\n",
    "            weight_decay=study.best_params[\"weight_decay\"],\n",
    "            optimizer_class=torch.optim.AdamW if study.best_params['optimizer'] == 'AdamW' else torch.optim.SGD,\n",
    "            loss_mix_ratio=study.best_params[\"loss_mix_ratio\"],\n",
    "            l1=study.best_params[\"l1\"],\n",
    "            class_weight=study.best_params['class_weight'],\n",
    "            device=device,\n",
    "            return_classifier=True,\n",
    "        )\n",
    "        classifiers_in_this_repeat.append((train_idx, classifier))\n",
    "\n",
    "        fold_records_f1[i] = val_f1\n",
    "        fold_records_acc[i] = val_acc\n",
    "\n",
    "    classifiers.append(classifiers_in_this_repeat)\n",
    "    avg_f1 = fold_records_f1.mean().item()\n",
    "    avg_acc = fold_records_acc.mean().item()\n",
    "\n",
    "    record[repeat] = [avg_f1, avg_acc]\n",
    "\n",
    "record.mean(axis=0), record.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1],\n",
       " ConvHead(\n",
       "   (layers): Sequential(\n",
       "     (0): ConvBlock(\n",
       "       (conv): Conv2d(8, 54, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "       (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (activation): SiLU()\n",
       "     )\n",
       "     (1): ConvBlock(\n",
       "       (conv): Conv2d(54, 54, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "       (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (activation): SiLU()\n",
       "     )\n",
       "     (2): Conv2d(54, 1, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat 0, trained on 1, and validated on 0\n",
    "classifiers[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.5551,  3.5551,  3.5551,  ...,  1.0584,  3.5551,  3.5551],\n",
       "          [ 3.5551,  3.5551,  3.5551,  ..., -0.1356,  3.5551,  3.5551],\n",
       "          [ 3.5551,  3.5551,  3.5551,  ..., -3.6323,  3.5551,  3.5551],\n",
       "          ...,\n",
       "          [ 3.5551,  3.5551,  3.5551,  ...,  3.5551,  3.5551,  3.5551],\n",
       "          [ 3.5551,  3.5551,  3.5551,  ...,  3.5551,  3.5551,  3.5551],\n",
       "          [ 3.5551,  3.5551,  3.5551,  ...,  3.5551,  3.5551,  3.5551]]]],\n",
       "       device='cuda:0', grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validated on 0\n",
    "classifiers[0][0][1](labeled_images[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the whole dataset\n",
    "\n",
    "classifiers = []\n",
    "record = np.zeros((10, 2))\n",
    "\n",
    "for repeat in range(10):\n",
    "    classifiers_in_this_repeat = []\n",
    "    # Get training and validation data\n",
    "    train_data = feature[0:2]\n",
    "    train_labels = labels[0:2]\n",
    "    val_data = feature[[2]]\n",
    "    val_labels = labels[[2]]\n",
    "\n",
    "    # Train and validate, get both F1 and accuracy\n",
    "    classifier, val_f1, val_acc = train_and_validate(\n",
    "        train_data=train_data,\n",
    "        train_labels=train_labels,\n",
    "        val_data=val_data,\n",
    "        val_labels=val_labels,\n",
    "        in_channels=feature.shape[1],\n",
    "        num_layers=study.best_params[\"num_layers\"],\n",
    "        kernel_size=study.best_params[\"kernel_size\"],\n",
    "        hidden_channels=study.best_params[\"hidden_channels\"],\n",
    "        epochs=study.best_params[\"epochs\"],\n",
    "        lr=study.best_params[\"lr\"],\n",
    "        weight_decay=study.best_params[\"weight_decay\"],\n",
    "        optimizer_class=torch.optim.AdamW if study.best_params['optimizer'] == 'AdamW' else torch.optim.SGD,\n",
    "        loss_mix_ratio=study.best_params[\"loss_mix_ratio\"],\n",
    "        l1=study.best_params[\"l1\"],\n",
    "        class_weight=study.best_params['class_weight'],\n",
    "        device=device,\n",
    "        return_classifier=True,\n",
    "    )\n",
    "    classifiers_in_this_repeat.append((train_idx, classifier))\n",
    "\n",
    "    record[repeat] = [val_f1.item(), val_acc.item()]\n",
    "\n",
    "    classifiers.append(classifiers_in_this_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.61045697, 0.61664719]), array([0.08809232, 0.08438048]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.mean(axis=0), record.std(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
