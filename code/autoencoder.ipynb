{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision\n",
    "from torchvision.models.efficientnet import MBConvConfig, FusedMBConvConfig\n",
    "\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling\")\n",
    "from preprocessing import to_NCHW, pad_to_384x384, standardize_images\n",
    "from autoencoder import EfficientNetEncoder, EfficientNetDecoder, AutoencoderConfig, masked_mse\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_amp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data = np.load(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/data/array_data.npz\")\n",
    "unlabeled_images, unlabeled_masks, labeled_images, labeled_masks, labels = data[\"unlabeled_images\"], data[\"unlabeled_masks\"], data[\"labeled_images\"], data[\"labeled_masks\"], data[\"labels\"]\n",
    "\n",
    "unlabeled_images = pad_to_384x384(to_NCHW(unlabeled_images))\n",
    "unlabeled_masks = pad_to_384x384(unlabeled_masks)\n",
    "\n",
    "labeled_images = pad_to_384x384(to_NCHW(labeled_images))\n",
    "labeled_masks = pad_to_384x384(labeled_masks)\n",
    "labels = pad_to_384x384(labels)\n",
    "\n",
    "# Convert to tensors and move to GPU\n",
    "unlabeled_images = torch.tensor(unlabeled_images, dtype=torch.float32).to(device)  # [161, 8, 384, 384]\n",
    "unlabeled_masks = torch.tensor(unlabeled_masks, dtype=torch.bool).to(device)    # [161, 384, 384]\n",
    "\n",
    "labeled_images = torch.tensor(labeled_images, dtype=torch.float32).to(device)      # [3, 8, 384, 384]\n",
    "labeled_masks = torch.tensor(labeled_masks, dtype=torch.bool).to(device)        # [3, 384, 384]\n",
    "labels = torch.tensor(labels, dtype=torch.long).to(device)                      # [3, 384, 384]\n",
    "\n",
    "\n",
    "# Standardize images\n",
    "unlabeled_images, std_channel, mean_channel = standardize_images(unlabeled_images, unlabeled_masks)\n",
    "labeled_images, _, _ = standardize_images(labeled_images, labeled_masks, std_channel, mean_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoencoderConfig([1, 1, 1], flip=True, rotate=True)\n"
     ]
    }
   ],
   "source": [
    "config = AutoencoderConfig(num_layers_block=[1, 1, 1], augmentation_flip=True, augmentation_rotate=True)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = []\n",
    "if config.augmentation_flip:\n",
    "    augmentation.append(torchvision.transforms.RandomHorizontalFlip(p=0.5))\n",
    "    augmentation.append(torchvision.transforms.RandomVerticalFlip(p=0.5))\n",
    "if config.augmentation_rotate:\n",
    "    augmentation.append(torchvision.transforms.RandomRotation(degrees=180, expand=True,\n",
    "                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR))\n",
    "    augmentation.append(torchvision.transforms.RandomCrop(size=384))\n",
    "augmentation = torchvision.transforms.Compose(augmentation)\n",
    "\n",
    "def apply_augment(images, masks, augmentation):\n",
    "    images_masks = torch.cat([masks.unsqueeze(1).float(), images], dim=1)\n",
    "    images_masks = [augmentation(image_mask) for image_mask in images_masks]\n",
    "    images_masks = torch.stack(images_masks)\n",
    "    return images_masks[:, 1:], images_masks[:, 0] > 0.5\n",
    "\n",
    "augment = lambda images, masks: apply_augment(images, masks, augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = [\n",
    "    FusedMBConvConfig(1, 3, 1, 16, 16, config.num_layers_block[0]),  # 384x384x8 -> 384x384x16\n",
    "    FusedMBConvConfig(4, 3, 2, 16, 32, config.num_layers_block[1]),  # 384x384x16 -> 192x192x32\n",
    "    MBConvConfig(4, 3, 2, 32, 64, config.num_layers_block[2]),       # 192x192x32 -> 96x96x64\n",
    "]\n",
    "\n",
    "# Build encoder and decoder\n",
    "encoder = EfficientNetEncoder(\n",
    "    inverted_residual_setting=encoder_config,\n",
    "    dropout=0.1,\n",
    "    input_channels=8,\n",
    "    last_channel=64,\n",
    ")\n",
    "\n",
    "decoder = EfficientNetDecoder()\n",
    "\n",
    "autoencoder = nn.Sequential(encoder, decoder).train().to(device)\n",
    "#compiled_autoencoder = torch.compile(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20000\n",
    "ckpt = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 20000]  # Checkpoints for saving model\n",
    "initial_lr = 1e-3  # Moderate starting LR for AdamW\n",
    "weight_decay = 1e-2  # Regularization for small dataset\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(autoencoder.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)  # Decay to near-zero\n",
    "scaler = torch.amp.GradScaler(device, enabled=use_amp)\n",
    "\n",
    "losses = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def trainer(images, masks, model, augment, optimizer, scheduler, scaler, loss_fn):\n",
    "    with torch.inference_mode():\n",
    "        images, masks = augment(images, masks)\n",
    "    images, masks = images.clone(), masks.clone()\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    with torch.amp.autocast(device, enabled=use_amp):\n",
    "        reconstructions = model(images)\n",
    "        loss = loss_fn(images, masks, reconstructions)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/ckpt\"\n",
    "os.makedirs(f\"{ckpt_path}/{str(config)}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0] or:\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0] to include these operations in the captured graph.\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0] \n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0] Graph break: from user code at:\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0]   File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torchvision/transforms/transforms.py\", line 1370, in forward\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0]     angle = self.get_params(self.degrees)\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0]   File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torchvision/transforms/transforms.py\", line 1352, in get_params\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0]     angle = float(torch.empty(1).uniform_(float(degrees[0]), float(degrees[1])).item())\n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0] \n",
      "W0308 02:40:33.321000 17407 site-packages/torch/_dynamo/variables/tensor.py:869] [8/0] \n",
      "W0308 02:40:53.721000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [25/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0308 02:40:53.721000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [25/8]    function: 'torch_dynamo_resume_in_rotate_at_665' (/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torchvision/transforms/_functional_tensor.py:665)\n",
      "W0308 02:40:53.721000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [25/8]    last reason: 25/0: L['oh'] == 492                                              \n",
      "W0308 02:40:53.721000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [25/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0308 02:40:53.721000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [25/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "W0308 02:41:20.600000 17407 site-packages/torch/_logging/_internal.py:1089] [38/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000 - Loss: 1.9808 - Time: 56.97s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20000 - Loss: 1.6613 - Time: 7.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20000 - Loss: 1.4127 - Time: 4.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20000 - Loss: 1.2305 - Time: 5.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20000 - Loss: 1.0779 - Time: 5.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20000 - Loss: 0.9597 - Time: 5.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20000 - Loss: 0.8502 - Time: 4.88s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\", \"L['self'].param_groups[0]['params'][10].grad\", \"L['self'].param_groups[0]['params'][11].grad\", \"L['self'].param_groups[0]['params'][12].grad\", \"L['self'].param_groups[0]['params'][13].grad\", \"L['self'].param_groups[0]['params'][14].grad\", \"L['self'].param_groups[0]['params'][15].grad\", \"L['self'].param_groups[0]['params'][16].grad\", \"L['self'].param_groups[0]['params'][17].grad\", \"L['self'].param_groups[0]['params'][18].grad\", \"L['self'].param_groups[0]['params'][19].grad\", \"L['self'].param_groups[0]['params'][20].grad\", \"L['self'].param_groups[0]['params'][21].grad\", \"L['self'].param_groups[0]['params'][22].grad\", \"L['self'].param_groups[0]['params'][23].grad\", \"L['self'].param_groups[0]['params'][24].grad\", \"L['self'].param_groups[0]['params'][25].grad\", \"L['self'].param_groups[0]['params'][26].grad\", \"L['self'].param_groups[0]['params'][27].grad\", \"L['self'].param_groups[0]['params'][28].grad\", \"L['self'].param_groups[0]['params'][29].grad\", \"L['self'].param_groups[0]['params'][30].grad\", \"L['self'].param_groups[0]['params'][31].grad\", \"L['self'].param_groups[0]['params'][32].grad\", \"L['self'].param_groups[0]['params'][33].grad\", \"L['self'].param_groups[0]['params'][34].grad\", \"L['self'].param_groups[0]['params'][35].grad\", \"L['self'].param_groups[0]['params'][36].grad\", \"L['self'].param_groups[0]['params'][37].grad\", \"L['self'].param_groups[0]['params'][38].grad\", \"L['self'].param_groups[0]['params'][39].grad\", \"L['self'].param_groups[0]['params'][40].grad\", \"L['self'].param_groups[0]['params'][41].grad\", \"L['self'].param_groups[0]['params'][42].grad\", \"L['self'].param_groups[0]['params'][43].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "W0308 02:42:06.484000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [39/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0308 02:42:06.484000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [39/8]    function: 'step' (/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/adamw.py:207)\n",
      "W0308 02:42:06.484000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [39/8]    last reason: 39/0: L['self'].param_groups[0]['lr'] == 0.001                    \n",
      "W0308 02:42:06.484000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [39/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0308 02:42:06.484000 17407 site-packages/torch/_dynamo/convert_frame.py:906] [39/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20000 - Loss: 0.7796 - Time: 5.86s\n",
      "Epoch 9/20000 - Loss: 0.7119 - Time: 0.22s\n",
      "Epoch 10/20000 - Loss: 0.6626 - Time: 0.19s\n",
      "Epoch 11/20000 - Loss: 0.6097 - Time: 0.20s\n",
      "Epoch 12/20000 - Loss: 0.5769 - Time: 0.20s\n",
      "Epoch 13/20000 - Loss: 0.5449 - Time: 0.20s\n",
      "Epoch 14/20000 - Loss: 0.5047 - Time: 0.19s\n",
      "Epoch 15/20000 - Loss: 0.4744 - Time: 0.20s\n",
      "Epoch 16/20000 - Loss: 0.4493 - Time: 0.19s\n",
      "Epoch 17/20000 - Loss: 0.4252 - Time: 0.19s\n",
      "Epoch 18/20000 - Loss: 0.4010 - Time: 0.19s\n",
      "Epoch 19/20000 - Loss: 0.3806 - Time: 0.20s\n",
      "Epoch 20/20000 - Loss: 0.3656 - Time: 0.19s\n",
      "Epoch 21/20000 - Loss: 0.3368 - Time: 0.20s\n",
      "Epoch 22/20000 - Loss: 0.3290 - Time: 0.19s\n",
      "Epoch 23/20000 - Loss: 0.3071 - Time: 0.20s\n",
      "Epoch 24/20000 - Loss: 0.2931 - Time: 0.19s\n",
      "Epoch 25/20000 - Loss: 0.2820 - Time: 0.20s\n",
      "Epoch 26/20000 - Loss: 0.2740 - Time: 0.19s\n",
      "Epoch 27/20000 - Loss: 0.2557 - Time: 0.20s\n",
      "Epoch 28/20000 - Loss: 0.2500 - Time: 0.20s\n",
      "Epoch 29/20000 - Loss: 0.2388 - Time: 0.20s\n",
      "Epoch 30/20000 - Loss: 0.2308 - Time: 0.19s\n",
      "Epoch 31/20000 - Loss: 0.2252 - Time: 0.19s\n",
      "Epoch 32/20000 - Loss: 0.2194 - Time: 0.19s\n",
      "Epoch 33/20000 - Loss: 0.2109 - Time: 0.20s\n",
      "Epoch 34/20000 - Loss: 0.2060 - Time: 0.20s\n",
      "Epoch 35/20000 - Loss: 0.2023 - Time: 0.19s\n",
      "Epoch 36/20000 - Loss: 0.1952 - Time: 0.19s\n",
      "Epoch 37/20000 - Loss: 0.1925 - Time: 0.20s\n",
      "Epoch 38/20000 - Loss: 0.1879 - Time: 0.19s\n",
      "Epoch 39/20000 - Loss: 0.1850 - Time: 0.20s\n",
      "Epoch 40/20000 - Loss: 0.1784 - Time: 0.19s\n",
      "Epoch 41/20000 - Loss: 0.1758 - Time: 0.19s\n",
      "Epoch 42/20000 - Loss: 0.1728 - Time: 0.19s\n",
      "Epoch 43/20000 - Loss: 0.1677 - Time: 0.20s\n",
      "Epoch 44/20000 - Loss: 0.1662 - Time: 0.20s\n",
      "Epoch 45/20000 - Loss: 0.1627 - Time: 0.19s\n",
      "Epoch 46/20000 - Loss: 0.1604 - Time: 0.19s\n",
      "Epoch 47/20000 - Loss: 0.1554 - Time: 0.19s\n",
      "Epoch 48/20000 - Loss: 0.1548 - Time: 0.19s\n",
      "Epoch 49/20000 - Loss: 0.1498 - Time: 0.20s\n",
      "Epoch 50/20000 - Loss: 0.1483 - Time: 0.19s\n",
      "Epoch 51/20000 - Loss: 0.1460 - Time: 0.20s\n",
      "Epoch 52/20000 - Loss: 0.1429 - Time: 0.20s\n",
      "Epoch 53/20000 - Loss: 0.1399 - Time: 0.19s\n",
      "Epoch 54/20000 - Loss: 0.1364 - Time: 0.19s\n",
      "Epoch 55/20000 - Loss: 0.1344 - Time: 0.19s\n",
      "Epoch 56/20000 - Loss: 0.1331 - Time: 0.19s\n",
      "Epoch 57/20000 - Loss: 0.1312 - Time: 0.19s\n",
      "Epoch 58/20000 - Loss: 0.1286 - Time: 0.19s\n",
      "Epoch 59/20000 - Loss: 0.1274 - Time: 0.20s\n",
      "Epoch 60/20000 - Loss: 0.1251 - Time: 0.19s\n",
      "Epoch 61/20000 - Loss: 0.1247 - Time: 0.19s\n",
      "Epoch 62/20000 - Loss: 0.1207 - Time: 0.19s\n",
      "Epoch 63/20000 - Loss: 0.1197 - Time: 0.20s\n",
      "Epoch 64/20000 - Loss: 0.1182 - Time: 0.19s\n",
      "Epoch 65/20000 - Loss: 0.1153 - Time: 0.19s\n",
      "Epoch 66/20000 - Loss: 0.1153 - Time: 0.20s\n",
      "Epoch 67/20000 - Loss: 0.1129 - Time: 0.19s\n",
      "Epoch 68/20000 - Loss: 0.1125 - Time: 0.19s\n",
      "Epoch 69/20000 - Loss: 0.1108 - Time: 0.20s\n",
      "Epoch 70/20000 - Loss: 0.1099 - Time: 0.20s\n",
      "Epoch 71/20000 - Loss: 0.1082 - Time: 0.19s\n",
      "Epoch 72/20000 - Loss: 0.1071 - Time: 0.20s\n",
      "Epoch 73/20000 - Loss: 0.1053 - Time: 0.20s\n",
      "Epoch 74/20000 - Loss: 0.1049 - Time: 0.20s\n",
      "Epoch 75/20000 - Loss: 0.1034 - Time: 0.20s\n",
      "Epoch 76/20000 - Loss: 0.1022 - Time: 0.20s\n",
      "Epoch 77/20000 - Loss: 0.1019 - Time: 0.20s\n",
      "Epoch 78/20000 - Loss: 0.0995 - Time: 0.20s\n",
      "Epoch 79/20000 - Loss: 0.0993 - Time: 0.20s\n",
      "Epoch 80/20000 - Loss: 0.0987 - Time: 0.20s\n",
      "Epoch 81/20000 - Loss: 0.0981 - Time: 0.20s\n",
      "Epoch 82/20000 - Loss: 0.0979 - Time: 0.20s\n",
      "Epoch 83/20000 - Loss: 0.0968 - Time: 0.20s\n",
      "Epoch 84/20000 - Loss: 0.0976 - Time: 0.20s\n",
      "Epoch 85/20000 - Loss: 0.0956 - Time: 0.20s\n",
      "Epoch 86/20000 - Loss: 0.0943 - Time: 0.20s\n",
      "Epoch 87/20000 - Loss: 0.0930 - Time: 0.20s\n",
      "Epoch 88/20000 - Loss: 0.0933 - Time: 0.20s\n",
      "Epoch 89/20000 - Loss: 0.0925 - Time: 0.20s\n",
      "Epoch 90/20000 - Loss: 0.0914 - Time: 0.20s\n",
      "Epoch 91/20000 - Loss: 0.0912 - Time: 0.20s\n",
      "Epoch 92/20000 - Loss: 0.0894 - Time: 0.20s\n",
      "Epoch 93/20000 - Loss: 0.0884 - Time: 0.20s\n",
      "Epoch 94/20000 - Loss: 0.0890 - Time: 0.20s\n",
      "Epoch 95/20000 - Loss: 0.0878 - Time: 0.20s\n",
      "Epoch 96/20000 - Loss: 0.0878 - Time: 0.20s\n",
      "Epoch 97/20000 - Loss: 0.0872 - Time: 0.20s\n",
      "Epoch 98/20000 - Loss: 0.0858 - Time: 0.20s\n",
      "Epoch 99/20000 - Loss: 0.0866 - Time: 0.20s\n",
      "Epoch 100/20000 - Loss: 0.0843 - Time: 0.20s\n",
      "Epoch 101/20000 - Loss: 0.0844 - Time: 0.20s\n",
      "Epoch 102/20000 - Loss: 0.0840 - Time: 0.20s\n",
      "Epoch 103/20000 - Loss: 0.0831 - Time: 0.20s\n",
      "Epoch 104/20000 - Loss: 0.0837 - Time: 0.20s\n",
      "Epoch 105/20000 - Loss: 0.0820 - Time: 0.20s\n",
      "Epoch 106/20000 - Loss: 0.0822 - Time: 0.20s\n",
      "Epoch 107/20000 - Loss: 0.0801 - Time: 0.20s\n",
      "Epoch 108/20000 - Loss: 0.0816 - Time: 0.20s\n",
      "Epoch 109/20000 - Loss: 0.0807 - Time: 0.19s\n",
      "Epoch 110/20000 - Loss: 0.0798 - Time: 0.19s\n",
      "Epoch 111/20000 - Loss: 0.0809 - Time: 0.20s\n",
      "Epoch 112/20000 - Loss: 0.0792 - Time: 0.20s\n",
      "Epoch 113/20000 - Loss: 0.0791 - Time: 0.19s\n",
      "Epoch 114/20000 - Loss: 0.0792 - Time: 0.20s\n",
      "Epoch 115/20000 - Loss: 0.0772 - Time: 0.19s\n",
      "Epoch 116/20000 - Loss: 0.0771 - Time: 0.19s\n",
      "Epoch 117/20000 - Loss: 0.0775 - Time: 0.19s\n",
      "Epoch 118/20000 - Loss: 0.0775 - Time: 0.19s\n",
      "Epoch 119/20000 - Loss: 0.0750 - Time: 0.19s\n",
      "Epoch 120/20000 - Loss: 0.0752 - Time: 0.19s\n",
      "Epoch 121/20000 - Loss: 0.0753 - Time: 0.19s\n",
      "Epoch 122/20000 - Loss: 0.0761 - Time: 0.19s\n",
      "Epoch 123/20000 - Loss: 0.0740 - Time: 0.19s\n",
      "Epoch 124/20000 - Loss: 0.0744 - Time: 0.20s\n",
      "Epoch 125/20000 - Loss: 0.0748 - Time: 0.19s\n",
      "Epoch 126/20000 - Loss: 0.0735 - Time: 0.20s\n",
      "Epoch 127/20000 - Loss: 0.0733 - Time: 0.19s\n",
      "Epoch 128/20000 - Loss: 0.0737 - Time: 0.19s\n",
      "Epoch 129/20000 - Loss: 0.0730 - Time: 0.20s\n",
      "Epoch 130/20000 - Loss: 0.0725 - Time: 0.20s\n",
      "Epoch 131/20000 - Loss: 0.0722 - Time: 0.19s\n",
      "Epoch 132/20000 - Loss: 0.0727 - Time: 0.19s\n",
      "Epoch 133/20000 - Loss: 0.0713 - Time: 0.20s\n",
      "Epoch 134/20000 - Loss: 0.0714 - Time: 0.20s\n",
      "Epoch 135/20000 - Loss: 0.0724 - Time: 0.19s\n",
      "Epoch 136/20000 - Loss: 0.0699 - Time: 0.19s\n",
      "Epoch 137/20000 - Loss: 0.0700 - Time: 0.19s\n",
      "Epoch 138/20000 - Loss: 0.0690 - Time: 0.20s\n",
      "Epoch 139/20000 - Loss: 0.0698 - Time: 0.19s\n",
      "Epoch 140/20000 - Loss: 0.0699 - Time: 0.20s\n",
      "Epoch 141/20000 - Loss: 0.0686 - Time: 0.19s\n",
      "Epoch 142/20000 - Loss: 0.0685 - Time: 0.19s\n",
      "Epoch 143/20000 - Loss: 0.0681 - Time: 0.19s\n",
      "Epoch 144/20000 - Loss: 0.0686 - Time: 0.19s\n",
      "Epoch 145/20000 - Loss: 0.0689 - Time: 0.19s\n",
      "Epoch 146/20000 - Loss: 0.0685 - Time: 0.19s\n",
      "Epoch 147/20000 - Loss: 0.0672 - Time: 0.19s\n",
      "Epoch 148/20000 - Loss: 0.0671 - Time: 0.19s\n",
      "Epoch 149/20000 - Loss: 0.0675 - Time: 0.19s\n",
      "Epoch 150/20000 - Loss: 0.0673 - Time: 0.19s\n",
      "Epoch 151/20000 - Loss: 0.0667 - Time: 0.20s\n",
      "Epoch 152/20000 - Loss: 0.0671 - Time: 0.19s\n",
      "Epoch 153/20000 - Loss: 0.0666 - Time: 0.19s\n",
      "Epoch 154/20000 - Loss: 0.0661 - Time: 0.19s\n",
      "Epoch 155/20000 - Loss: 0.0657 - Time: 0.19s\n",
      "Epoch 156/20000 - Loss: 0.0661 - Time: 0.19s\n",
      "Epoch 157/20000 - Loss: 0.0662 - Time: 0.19s\n",
      "Epoch 158/20000 - Loss: 0.0655 - Time: 0.19s\n",
      "Epoch 159/20000 - Loss: 0.0657 - Time: 0.19s\n",
      "Epoch 160/20000 - Loss: 0.0656 - Time: 0.19s\n",
      "Epoch 161/20000 - Loss: 0.0652 - Time: 0.19s\n",
      "Epoch 162/20000 - Loss: 0.0646 - Time: 0.19s\n",
      "Epoch 163/20000 - Loss: 0.0645 - Time: 0.19s\n",
      "Epoch 164/20000 - Loss: 0.0645 - Time: 0.19s\n",
      "Epoch 165/20000 - Loss: 0.0629 - Time: 0.19s\n",
      "Epoch 166/20000 - Loss: 0.0651 - Time: 0.19s\n",
      "Epoch 167/20000 - Loss: 0.0638 - Time: 0.19s\n",
      "Epoch 168/20000 - Loss: 0.0624 - Time: 0.19s\n",
      "Epoch 169/20000 - Loss: 0.0630 - Time: 0.19s\n",
      "Epoch 170/20000 - Loss: 0.0619 - Time: 0.19s\n",
      "Epoch 171/20000 - Loss: 0.0630 - Time: 0.19s\n",
      "Epoch 172/20000 - Loss: 0.0617 - Time: 0.19s\n",
      "Epoch 173/20000 - Loss: 0.0622 - Time: 0.19s\n",
      "Epoch 174/20000 - Loss: 0.0622 - Time: 0.19s\n",
      "Epoch 175/20000 - Loss: 0.0619 - Time: 0.19s\n",
      "Epoch 176/20000 - Loss: 0.0616 - Time: 0.19s\n",
      "Epoch 177/20000 - Loss: 0.0623 - Time: 0.19s\n",
      "Epoch 178/20000 - Loss: 0.0610 - Time: 0.19s\n",
      "Epoch 179/20000 - Loss: 0.0613 - Time: 0.19s\n",
      "Epoch 180/20000 - Loss: 0.0617 - Time: 0.19s\n",
      "Epoch 181/20000 - Loss: 0.0609 - Time: 0.19s\n",
      "Epoch 182/20000 - Loss: 0.0615 - Time: 0.19s\n",
      "Epoch 183/20000 - Loss: 0.0607 - Time: 0.19s\n",
      "Epoch 184/20000 - Loss: 0.0602 - Time: 0.19s\n",
      "Epoch 185/20000 - Loss: 0.0600 - Time: 0.19s\n",
      "Epoch 186/20000 - Loss: 0.0597 - Time: 0.19s\n",
      "Epoch 187/20000 - Loss: 0.0603 - Time: 0.19s\n",
      "Epoch 188/20000 - Loss: 0.0599 - Time: 0.19s\n",
      "Epoch 189/20000 - Loss: 0.0589 - Time: 0.19s\n",
      "Epoch 190/20000 - Loss: 0.0602 - Time: 0.19s\n",
      "Epoch 191/20000 - Loss: 0.0601 - Time: 0.19s\n",
      "Epoch 192/20000 - Loss: 0.0589 - Time: 0.19s\n",
      "Epoch 193/20000 - Loss: 0.0589 - Time: 0.19s\n",
      "Epoch 194/20000 - Loss: 0.0588 - Time: 0.19s\n",
      "Epoch 195/20000 - Loss: 0.0592 - Time: 0.19s\n",
      "Epoch 196/20000 - Loss: 0.0583 - Time: 0.19s\n",
      "Epoch 197/20000 - Loss: 0.0579 - Time: 0.19s\n",
      "Epoch 198/20000 - Loss: 0.0579 - Time: 0.19s\n",
      "Epoch 199/20000 - Loss: 0.0586 - Time: 0.19s\n",
      "Epoch 200/20000 - Loss: 0.0589 - Time: 0.20s\n",
      "Epoch 201/20000 - Loss: 0.0579 - Time: 0.19s\n",
      "Epoch 202/20000 - Loss: 0.0576 - Time: 0.19s\n",
      "Epoch 203/20000 - Loss: 0.0574 - Time: 0.19s\n",
      "Epoch 204/20000 - Loss: 0.0576 - Time: 0.19s\n",
      "Epoch 205/20000 - Loss: 0.0571 - Time: 0.19s\n",
      "Epoch 206/20000 - Loss: 0.0573 - Time: 0.19s\n",
      "Epoch 207/20000 - Loss: 0.0567 - Time: 0.19s\n",
      "Epoch 208/20000 - Loss: 0.0561 - Time: 0.19s\n",
      "Epoch 209/20000 - Loss: 0.0580 - Time: 0.19s\n",
      "Epoch 210/20000 - Loss: 0.0565 - Time: 0.19s\n",
      "Epoch 211/20000 - Loss: 0.0565 - Time: 0.19s\n",
      "Epoch 212/20000 - Loss: 0.0564 - Time: 0.19s\n",
      "Epoch 213/20000 - Loss: 0.0558 - Time: 0.19s\n",
      "Epoch 214/20000 - Loss: 0.0559 - Time: 0.20s\n",
      "Epoch 215/20000 - Loss: 0.0557 - Time: 0.19s\n",
      "Epoch 216/20000 - Loss: 0.0555 - Time: 0.20s\n",
      "Epoch 217/20000 - Loss: 0.0552 - Time: 0.20s\n",
      "Epoch 218/20000 - Loss: 0.0560 - Time: 0.19s\n",
      "Epoch 219/20000 - Loss: 0.0558 - Time: 0.20s\n",
      "Epoch 220/20000 - Loss: 0.0557 - Time: 0.19s\n",
      "Epoch 221/20000 - Loss: 0.0551 - Time: 0.19s\n",
      "Epoch 222/20000 - Loss: 0.0548 - Time: 0.19s\n",
      "Epoch 223/20000 - Loss: 0.0552 - Time: 0.19s\n",
      "Epoch 224/20000 - Loss: 0.0553 - Time: 0.19s\n",
      "Epoch 225/20000 - Loss: 0.0549 - Time: 0.19s\n",
      "Epoch 226/20000 - Loss: 0.0550 - Time: 0.19s\n",
      "Epoch 227/20000 - Loss: 0.0540 - Time: 0.19s\n",
      "Epoch 228/20000 - Loss: 0.0548 - Time: 0.19s\n",
      "Epoch 229/20000 - Loss: 0.0537 - Time: 0.19s\n",
      "Epoch 230/20000 - Loss: 0.0542 - Time: 0.19s\n",
      "Epoch 231/20000 - Loss: 0.0541 - Time: 0.19s\n",
      "Epoch 232/20000 - Loss: 0.0537 - Time: 0.19s\n",
      "Epoch 233/20000 - Loss: 0.0536 - Time: 0.19s\n",
      "Epoch 234/20000 - Loss: 0.0532 - Time: 0.19s\n",
      "Epoch 235/20000 - Loss: 0.0534 - Time: 0.19s\n",
      "Epoch 236/20000 - Loss: 0.0540 - Time: 0.19s\n",
      "Epoch 237/20000 - Loss: 0.0534 - Time: 0.19s\n",
      "Epoch 238/20000 - Loss: 0.0534 - Time: 0.19s\n",
      "Epoch 239/20000 - Loss: 0.0524 - Time: 0.19s\n",
      "Epoch 240/20000 - Loss: 0.0528 - Time: 0.19s\n",
      "Epoch 241/20000 - Loss: 0.0530 - Time: 0.19s\n",
      "Epoch 242/20000 - Loss: 0.0523 - Time: 0.19s\n",
      "Epoch 243/20000 - Loss: 0.0523 - Time: 0.19s\n",
      "Epoch 244/20000 - Loss: 0.0520 - Time: 0.19s\n",
      "Epoch 245/20000 - Loss: 0.0515 - Time: 0.19s\n",
      "Epoch 246/20000 - Loss: 0.0527 - Time: 0.19s\n",
      "Epoch 247/20000 - Loss: 0.0522 - Time: 0.19s\n",
      "Epoch 248/20000 - Loss: 0.0519 - Time: 0.19s\n",
      "Epoch 249/20000 - Loss: 0.0524 - Time: 0.19s\n",
      "Epoch 250/20000 - Loss: 0.0513 - Time: 0.19s\n",
      "Epoch 251/20000 - Loss: 0.0519 - Time: 0.19s\n",
      "Epoch 252/20000 - Loss: 0.0514 - Time: 0.19s\n",
      "Epoch 253/20000 - Loss: 0.0518 - Time: 0.19s\n",
      "Epoch 254/20000 - Loss: 0.0516 - Time: 0.20s\n",
      "Epoch 255/20000 - Loss: 0.0513 - Time: 0.19s\n",
      "Epoch 256/20000 - Loss: 0.0509 - Time: 0.20s\n",
      "Epoch 257/20000 - Loss: 0.0506 - Time: 0.20s\n",
      "Epoch 258/20000 - Loss: 0.0510 - Time: 0.20s\n",
      "Epoch 259/20000 - Loss: 0.0501 - Time: 0.20s\n",
      "Epoch 260/20000 - Loss: 0.0502 - Time: 0.20s\n",
      "Epoch 261/20000 - Loss: 0.0504 - Time: 0.20s\n",
      "Epoch 262/20000 - Loss: 0.0498 - Time: 0.20s\n",
      "Epoch 263/20000 - Loss: 0.0509 - Time: 0.20s\n",
      "Epoch 264/20000 - Loss: 0.0503 - Time: 0.20s\n",
      "Epoch 265/20000 - Loss: 0.0508 - Time: 0.20s\n",
      "Epoch 266/20000 - Loss: 0.0502 - Time: 0.20s\n",
      "Epoch 267/20000 - Loss: 0.0498 - Time: 0.20s\n",
      "Epoch 268/20000 - Loss: 0.0493 - Time: 0.20s\n",
      "Epoch 269/20000 - Loss: 0.0502 - Time: 0.20s\n",
      "Epoch 270/20000 - Loss: 0.0495 - Time: 0.20s\n",
      "Epoch 271/20000 - Loss: 0.0496 - Time: 0.20s\n",
      "Epoch 272/20000 - Loss: 0.0493 - Time: 0.20s\n",
      "Epoch 273/20000 - Loss: 0.0499 - Time: 0.20s\n",
      "Epoch 274/20000 - Loss: 0.0491 - Time: 0.20s\n",
      "Epoch 275/20000 - Loss: 0.0493 - Time: 0.20s\n",
      "Epoch 276/20000 - Loss: 0.0499 - Time: 0.20s\n",
      "Epoch 277/20000 - Loss: 0.0488 - Time: 0.20s\n",
      "Epoch 278/20000 - Loss: 0.0495 - Time: 0.20s\n",
      "Epoch 279/20000 - Loss: 0.0484 - Time: 0.20s\n",
      "Epoch 280/20000 - Loss: 0.0490 - Time: 0.20s\n",
      "Epoch 281/20000 - Loss: 0.0484 - Time: 0.20s\n",
      "Epoch 282/20000 - Loss: 0.0488 - Time: 0.20s\n",
      "Epoch 283/20000 - Loss: 0.0496 - Time: 0.20s\n",
      "Epoch 284/20000 - Loss: 0.0484 - Time: 0.20s\n",
      "Epoch 285/20000 - Loss: 0.0481 - Time: 0.19s\n",
      "Epoch 286/20000 - Loss: 0.0482 - Time: 0.20s\n",
      "Epoch 287/20000 - Loss: 0.0481 - Time: 0.20s\n",
      "Epoch 288/20000 - Loss: 0.0478 - Time: 0.20s\n",
      "Epoch 289/20000 - Loss: 0.0485 - Time: 0.19s\n",
      "Epoch 290/20000 - Loss: 0.0475 - Time: 0.19s\n",
      "Epoch 291/20000 - Loss: 0.0480 - Time: 0.19s\n",
      "Epoch 292/20000 - Loss: 0.0478 - Time: 0.20s\n",
      "Epoch 293/20000 - Loss: 0.0479 - Time: 0.19s\n",
      "Epoch 294/20000 - Loss: 0.0474 - Time: 0.20s\n",
      "Epoch 295/20000 - Loss: 0.0475 - Time: 0.19s\n",
      "Epoch 296/20000 - Loss: 0.0477 - Time: 0.20s\n",
      "Epoch 297/20000 - Loss: 0.0472 - Time: 0.20s\n",
      "Epoch 298/20000 - Loss: 0.0468 - Time: 0.19s\n",
      "Epoch 299/20000 - Loss: 0.0474 - Time: 0.19s\n",
      "Epoch 300/20000 - Loss: 0.0467 - Time: 0.20s\n",
      "Epoch 301/20000 - Loss: 0.0466 - Time: 0.20s\n",
      "Epoch 302/20000 - Loss: 0.0465 - Time: 0.20s\n",
      "Epoch 303/20000 - Loss: 0.0467 - Time: 0.19s\n",
      "Epoch 304/20000 - Loss: 0.0460 - Time: 0.20s\n",
      "Epoch 305/20000 - Loss: 0.0462 - Time: 0.19s\n",
      "Epoch 306/20000 - Loss: 0.0464 - Time: 0.20s\n",
      "Epoch 307/20000 - Loss: 0.0463 - Time: 0.19s\n",
      "Epoch 308/20000 - Loss: 0.0468 - Time: 0.20s\n",
      "Epoch 309/20000 - Loss: 0.0463 - Time: 0.20s\n",
      "Epoch 310/20000 - Loss: 0.0463 - Time: 0.20s\n",
      "Epoch 311/20000 - Loss: 0.0470 - Time: 0.20s\n",
      "Epoch 312/20000 - Loss: 0.0470 - Time: 0.20s\n",
      "Epoch 313/20000 - Loss: 0.0467 - Time: 0.20s\n",
      "Epoch 314/20000 - Loss: 0.0460 - Time: 0.20s\n",
      "Epoch 315/20000 - Loss: 0.0462 - Time: 0.20s\n",
      "Epoch 316/20000 - Loss: 0.0460 - Time: 0.19s\n",
      "Epoch 317/20000 - Loss: 0.0457 - Time: 0.19s\n",
      "Epoch 318/20000 - Loss: 0.0453 - Time: 0.19s\n",
      "Epoch 319/20000 - Loss: 0.0452 - Time: 0.19s\n",
      "Epoch 320/20000 - Loss: 0.0457 - Time: 0.19s\n",
      "Epoch 321/20000 - Loss: 0.0447 - Time: 0.19s\n",
      "Epoch 322/20000 - Loss: 0.0454 - Time: 0.19s\n",
      "Epoch 323/20000 - Loss: 0.0460 - Time: 0.19s\n",
      "Epoch 324/20000 - Loss: 0.0450 - Time: 0.19s\n",
      "Epoch 325/20000 - Loss: 0.0448 - Time: 0.20s\n",
      "Epoch 326/20000 - Loss: 0.0456 - Time: 0.19s\n",
      "Epoch 327/20000 - Loss: 0.0450 - Time: 0.19s\n",
      "Epoch 328/20000 - Loss: 0.0454 - Time: 0.20s\n",
      "Epoch 329/20000 - Loss: 0.0451 - Time: 0.19s\n",
      "Epoch 330/20000 - Loss: 0.0447 - Time: 0.19s\n",
      "Epoch 331/20000 - Loss: 0.0444 - Time: 0.19s\n",
      "Epoch 332/20000 - Loss: 0.0445 - Time: 0.19s\n",
      "Epoch 333/20000 - Loss: 0.0440 - Time: 0.19s\n",
      "Epoch 334/20000 - Loss: 0.0450 - Time: 0.20s\n",
      "Epoch 335/20000 - Loss: 0.0444 - Time: 0.19s\n",
      "Epoch 336/20000 - Loss: 0.0441 - Time: 0.19s\n",
      "Epoch 337/20000 - Loss: 0.0441 - Time: 0.19s\n",
      "Epoch 338/20000 - Loss: 0.0436 - Time: 0.19s\n",
      "Epoch 339/20000 - Loss: 0.0441 - Time: 0.19s\n",
      "Epoch 340/20000 - Loss: 0.0439 - Time: 0.19s\n",
      "Epoch 341/20000 - Loss: 0.0435 - Time: 0.19s\n",
      "Epoch 342/20000 - Loss: 0.0434 - Time: 0.19s\n",
      "Epoch 343/20000 - Loss: 0.0440 - Time: 0.19s\n",
      "Epoch 344/20000 - Loss: 0.0434 - Time: 0.19s\n",
      "Epoch 345/20000 - Loss: 0.0436 - Time: 0.19s\n",
      "Epoch 346/20000 - Loss: 0.0441 - Time: 0.19s\n",
      "Epoch 347/20000 - Loss: 0.0436 - Time: 0.19s\n",
      "Epoch 348/20000 - Loss: 0.0437 - Time: 0.19s\n",
      "Epoch 349/20000 - Loss: 0.0430 - Time: 0.19s\n",
      "Epoch 350/20000 - Loss: 0.0434 - Time: 0.19s\n",
      "Epoch 351/20000 - Loss: 0.0436 - Time: 0.19s\n",
      "Epoch 352/20000 - Loss: 0.0432 - Time: 0.19s\n",
      "Epoch 353/20000 - Loss: 0.0431 - Time: 0.19s\n",
      "Epoch 354/20000 - Loss: 0.0433 - Time: 0.19s\n",
      "Epoch 355/20000 - Loss: 0.0435 - Time: 0.19s\n",
      "Epoch 356/20000 - Loss: 0.0423 - Time: 0.19s\n",
      "Epoch 357/20000 - Loss: 0.0430 - Time: 0.19s\n",
      "Epoch 358/20000 - Loss: 0.0436 - Time: 0.19s\n",
      "Epoch 359/20000 - Loss: 0.0437 - Time: 0.19s\n",
      "Epoch 360/20000 - Loss: 0.0422 - Time: 0.19s\n",
      "Epoch 361/20000 - Loss: 0.0426 - Time: 0.19s\n",
      "Epoch 362/20000 - Loss: 0.0424 - Time: 0.19s\n",
      "Epoch 363/20000 - Loss: 0.0428 - Time: 0.20s\n",
      "Epoch 364/20000 - Loss: 0.0427 - Time: 0.19s\n",
      "Epoch 365/20000 - Loss: 0.0421 - Time: 0.19s\n",
      "Epoch 366/20000 - Loss: 0.0421 - Time: 0.19s\n",
      "Epoch 367/20000 - Loss: 0.0421 - Time: 0.19s\n",
      "Epoch 368/20000 - Loss: 0.0413 - Time: 0.19s\n",
      "Epoch 369/20000 - Loss: 0.0427 - Time: 0.20s\n",
      "Epoch 370/20000 - Loss: 0.0423 - Time: 0.19s\n",
      "Epoch 371/20000 - Loss: 0.0420 - Time: 0.19s\n",
      "Epoch 372/20000 - Loss: 0.0424 - Time: 0.19s\n",
      "Epoch 373/20000 - Loss: 0.0419 - Time: 0.19s\n",
      "Epoch 374/20000 - Loss: 0.0419 - Time: 0.19s\n",
      "Epoch 375/20000 - Loss: 0.0412 - Time: 0.20s\n",
      "Epoch 376/20000 - Loss: 0.0419 - Time: 0.20s\n",
      "Epoch 377/20000 - Loss: 0.0412 - Time: 0.19s\n",
      "Epoch 378/20000 - Loss: 0.0424 - Time: 0.19s\n",
      "Epoch 379/20000 - Loss: 0.0418 - Time: 0.19s\n",
      "Epoch 380/20000 - Loss: 0.0415 - Time: 0.19s\n",
      "Epoch 381/20000 - Loss: 0.0408 - Time: 0.19s\n",
      "Epoch 382/20000 - Loss: 0.0418 - Time: 0.20s\n",
      "Epoch 383/20000 - Loss: 0.0417 - Time: 0.19s\n",
      "Epoch 384/20000 - Loss: 0.0417 - Time: 0.19s\n",
      "Epoch 385/20000 - Loss: 0.0411 - Time: 0.19s\n",
      "Epoch 386/20000 - Loss: 0.0422 - Time: 0.20s\n",
      "Epoch 387/20000 - Loss: 0.0416 - Time: 0.19s\n",
      "Epoch 388/20000 - Loss: 0.0414 - Time: 0.19s\n",
      "Epoch 389/20000 - Loss: 0.0411 - Time: 0.19s\n",
      "Epoch 390/20000 - Loss: 0.0405 - Time: 0.19s\n",
      "Epoch 391/20000 - Loss: 0.0403 - Time: 0.19s\n",
      "Epoch 392/20000 - Loss: 0.0410 - Time: 0.20s\n",
      "Epoch 393/20000 - Loss: 0.0404 - Time: 0.19s\n",
      "Epoch 394/20000 - Loss: 0.0403 - Time: 0.19s\n",
      "Epoch 395/20000 - Loss: 0.0402 - Time: 0.19s\n",
      "Epoch 396/20000 - Loss: 0.0405 - Time: 0.19s\n",
      "Epoch 397/20000 - Loss: 0.0395 - Time: 0.19s\n",
      "Epoch 398/20000 - Loss: 0.0405 - Time: 0.19s\n",
      "Epoch 399/20000 - Loss: 0.0403 - Time: 0.19s\n",
      "Epoch 400/20000 - Loss: 0.0405 - Time: 0.20s\n",
      "Epoch 401/20000 - Loss: 0.0401 - Time: 0.19s\n",
      "Epoch 402/20000 - Loss: 0.0404 - Time: 0.19s\n",
      "Epoch 403/20000 - Loss: 0.0405 - Time: 0.19s\n",
      "Epoch 404/20000 - Loss: 0.0399 - Time: 0.20s\n",
      "Epoch 405/20000 - Loss: 0.0397 - Time: 0.19s\n",
      "Epoch 406/20000 - Loss: 0.0405 - Time: 0.20s\n",
      "Epoch 407/20000 - Loss: 0.0399 - Time: 0.19s\n",
      "Epoch 408/20000 - Loss: 0.0399 - Time: 0.20s\n",
      "Epoch 409/20000 - Loss: 0.0399 - Time: 0.20s\n",
      "Epoch 410/20000 - Loss: 0.0392 - Time: 0.19s\n",
      "Epoch 411/20000 - Loss: 0.0398 - Time: 0.19s\n",
      "Epoch 412/20000 - Loss: 0.0394 - Time: 0.19s\n",
      "Epoch 413/20000 - Loss: 0.0394 - Time: 0.19s\n",
      "Epoch 414/20000 - Loss: 0.0400 - Time: 0.20s\n",
      "Epoch 415/20000 - Loss: 0.0388 - Time: 0.20s\n",
      "Epoch 416/20000 - Loss: 0.0397 - Time: 0.19s\n",
      "Epoch 417/20000 - Loss: 0.0392 - Time: 0.19s\n",
      "Epoch 418/20000 - Loss: 0.0397 - Time: 0.20s\n",
      "Epoch 419/20000 - Loss: 0.0391 - Time: 0.20s\n",
      "Epoch 420/20000 - Loss: 0.0410 - Time: 0.20s\n",
      "Epoch 421/20000 - Loss: 0.0393 - Time: 0.20s\n",
      "Epoch 422/20000 - Loss: 0.0386 - Time: 0.20s\n",
      "Epoch 423/20000 - Loss: 0.0409 - Time: 0.20s\n",
      "Epoch 424/20000 - Loss: 0.0400 - Time: 0.20s\n",
      "Epoch 425/20000 - Loss: 0.0389 - Time: 0.20s\n",
      "Epoch 426/20000 - Loss: 0.0393 - Time: 0.20s\n",
      "Epoch 427/20000 - Loss: 0.0391 - Time: 0.19s\n",
      "Epoch 428/20000 - Loss: 0.0388 - Time: 0.20s\n",
      "Epoch 429/20000 - Loss: 0.0388 - Time: 0.20s\n",
      "Epoch 430/20000 - Loss: 0.0390 - Time: 0.20s\n",
      "Epoch 431/20000 - Loss: 0.0386 - Time: 0.20s\n",
      "Epoch 432/20000 - Loss: 0.0388 - Time: 0.19s\n",
      "Epoch 433/20000 - Loss: 0.0383 - Time: 0.20s\n",
      "Epoch 434/20000 - Loss: 0.0392 - Time: 0.20s\n",
      "Epoch 435/20000 - Loss: 0.0384 - Time: 0.20s\n",
      "Epoch 436/20000 - Loss: 0.0384 - Time: 0.20s\n",
      "Epoch 437/20000 - Loss: 0.0385 - Time: 0.20s\n",
      "Epoch 438/20000 - Loss: 0.0385 - Time: 0.20s\n",
      "Epoch 439/20000 - Loss: 0.0384 - Time: 0.19s\n",
      "Epoch 440/20000 - Loss: 0.0382 - Time: 0.19s\n",
      "Epoch 441/20000 - Loss: 0.0381 - Time: 0.19s\n",
      "Epoch 442/20000 - Loss: 0.0388 - Time: 0.20s\n",
      "Epoch 443/20000 - Loss: 0.0382 - Time: 0.19s\n",
      "Epoch 444/20000 - Loss: 0.0384 - Time: 0.19s\n",
      "Epoch 445/20000 - Loss: 0.0392 - Time: 0.19s\n",
      "Epoch 446/20000 - Loss: 0.0377 - Time: 0.19s\n",
      "Epoch 447/20000 - Loss: 0.0378 - Time: 0.19s\n",
      "Epoch 448/20000 - Loss: 0.0377 - Time: 0.19s\n",
      "Epoch 449/20000 - Loss: 0.0380 - Time: 0.20s\n",
      "Epoch 450/20000 - Loss: 0.0382 - Time: 0.19s\n",
      "Epoch 451/20000 - Loss: 0.0377 - Time: 0.19s\n",
      "Epoch 452/20000 - Loss: 0.0386 - Time: 0.19s\n",
      "Epoch 453/20000 - Loss: 0.0378 - Time: 0.19s\n",
      "Epoch 454/20000 - Loss: 0.0382 - Time: 0.19s\n",
      "Epoch 455/20000 - Loss: 0.0374 - Time: 0.19s\n",
      "Epoch 456/20000 - Loss: 0.0377 - Time: 0.19s\n",
      "Epoch 457/20000 - Loss: 0.0381 - Time: 0.19s\n",
      "Epoch 458/20000 - Loss: 0.0373 - Time: 0.20s\n",
      "Epoch 459/20000 - Loss: 0.0370 - Time: 0.19s\n",
      "Epoch 460/20000 - Loss: 0.0374 - Time: 0.19s\n",
      "Epoch 461/20000 - Loss: 0.0371 - Time: 0.19s\n",
      "Epoch 462/20000 - Loss: 0.0371 - Time: 0.19s\n",
      "Epoch 463/20000 - Loss: 0.0377 - Time: 0.19s\n",
      "Epoch 464/20000 - Loss: 0.0372 - Time: 0.19s\n",
      "Epoch 465/20000 - Loss: 0.0371 - Time: 0.19s\n",
      "Epoch 466/20000 - Loss: 0.0371 - Time: 0.19s\n",
      "Epoch 467/20000 - Loss: 0.0373 - Time: 0.19s\n",
      "Epoch 468/20000 - Loss: 0.0368 - Time: 0.19s\n",
      "Epoch 469/20000 - Loss: 0.0368 - Time: 0.19s\n",
      "Epoch 470/20000 - Loss: 0.0368 - Time: 0.19s\n",
      "Epoch 471/20000 - Loss: 0.0368 - Time: 0.19s\n",
      "Epoch 472/20000 - Loss: 0.0367 - Time: 0.19s\n",
      "Epoch 473/20000 - Loss: 0.0366 - Time: 0.20s\n",
      "Epoch 474/20000 - Loss: 0.0370 - Time: 0.19s\n",
      "Epoch 475/20000 - Loss: 0.0377 - Time: 0.19s\n",
      "Epoch 476/20000 - Loss: 0.0374 - Time: 0.19s\n",
      "Epoch 477/20000 - Loss: 0.0366 - Time: 0.19s\n",
      "Epoch 478/20000 - Loss: 0.0372 - Time: 0.20s\n",
      "Epoch 479/20000 - Loss: 0.0375 - Time: 0.19s\n",
      "Epoch 480/20000 - Loss: 0.0366 - Time: 0.20s\n",
      "Epoch 481/20000 - Loss: 0.0371 - Time: 0.20s\n",
      "Epoch 482/20000 - Loss: 0.0367 - Time: 0.19s\n",
      "Epoch 483/20000 - Loss: 0.0371 - Time: 0.19s\n",
      "Epoch 484/20000 - Loss: 0.0386 - Time: 0.19s\n",
      "Epoch 485/20000 - Loss: 0.0372 - Time: 0.19s\n",
      "Epoch 486/20000 - Loss: 0.0372 - Time: 0.19s\n",
      "Epoch 487/20000 - Loss: 0.0377 - Time: 0.19s\n",
      "Epoch 488/20000 - Loss: 0.0364 - Time: 0.19s\n",
      "Epoch 489/20000 - Loss: 0.0370 - Time: 0.19s\n",
      "Epoch 490/20000 - Loss: 0.0365 - Time: 0.19s\n",
      "Epoch 491/20000 - Loss: 0.0374 - Time: 0.19s\n",
      "Epoch 492/20000 - Loss: 0.0368 - Time: 0.19s\n",
      "Epoch 493/20000 - Loss: 0.0360 - Time: 0.19s\n",
      "Epoch 494/20000 - Loss: 0.0367 - Time: 0.20s\n",
      "Epoch 495/20000 - Loss: 0.0363 - Time: 0.19s\n",
      "Epoch 496/20000 - Loss: 0.0366 - Time: 0.19s\n",
      "Epoch 497/20000 - Loss: 0.0371 - Time: 0.19s\n",
      "Epoch 498/20000 - Loss: 0.0366 - Time: 0.19s\n",
      "Epoch 499/20000 - Loss: 0.0376 - Time: 0.20s\n",
      "Epoch 500/20000 - Loss: 0.0367 - Time: 0.19s\n",
      "Epoch 501/20000 - Loss: 0.0362 - Time: 0.19s\n",
      "Epoch 502/20000 - Loss: 0.0366 - Time: 0.19s\n",
      "Epoch 503/20000 - Loss: 0.0357 - Time: 0.19s\n",
      "Epoch 504/20000 - Loss: 0.0359 - Time: 0.20s\n",
      "Epoch 505/20000 - Loss: 0.0355 - Time: 0.19s\n",
      "Epoch 506/20000 - Loss: 0.0362 - Time: 0.19s\n",
      "Epoch 507/20000 - Loss: 0.0357 - Time: 0.19s\n",
      "Epoch 508/20000 - Loss: 0.0360 - Time: 0.19s\n",
      "Epoch 509/20000 - Loss: 0.0354 - Time: 0.19s\n",
      "Epoch 510/20000 - Loss: 0.0364 - Time: 0.19s\n",
      "Epoch 511/20000 - Loss: 0.0357 - Time: 0.19s\n",
      "Epoch 512/20000 - Loss: 0.0350 - Time: 0.19s\n",
      "Epoch 513/20000 - Loss: 0.0370 - Time: 0.19s\n",
      "Epoch 514/20000 - Loss: 0.0354 - Time: 0.20s\n",
      "Epoch 515/20000 - Loss: 0.0353 - Time: 0.19s\n",
      "Epoch 516/20000 - Loss: 0.0354 - Time: 0.19s\n",
      "Epoch 517/20000 - Loss: 0.0354 - Time: 0.19s\n",
      "Epoch 518/20000 - Loss: 0.0354 - Time: 0.19s\n",
      "Epoch 519/20000 - Loss: 0.0353 - Time: 0.19s\n",
      "Epoch 520/20000 - Loss: 0.0354 - Time: 0.19s\n",
      "Epoch 521/20000 - Loss: 0.0351 - Time: 0.19s\n",
      "Epoch 522/20000 - Loss: 0.0353 - Time: 0.19s\n",
      "Epoch 523/20000 - Loss: 0.0349 - Time: 0.19s\n",
      "Epoch 524/20000 - Loss: 0.0345 - Time: 0.19s\n",
      "Epoch 525/20000 - Loss: 0.0352 - Time: 0.19s\n",
      "Epoch 526/20000 - Loss: 0.0349 - Time: 0.20s\n",
      "Epoch 527/20000 - Loss: 0.0348 - Time: 0.19s\n",
      "Epoch 528/20000 - Loss: 0.0350 - Time: 0.19s\n",
      "Epoch 529/20000 - Loss: 0.0349 - Time: 0.19s\n",
      "Epoch 530/20000 - Loss: 0.0352 - Time: 0.19s\n",
      "Epoch 531/20000 - Loss: 0.0345 - Time: 0.19s\n",
      "Epoch 532/20000 - Loss: 0.0348 - Time: 0.19s\n",
      "Epoch 533/20000 - Loss: 0.0347 - Time: 0.20s\n",
      "Epoch 534/20000 - Loss: 0.0348 - Time: 0.19s\n",
      "Epoch 535/20000 - Loss: 0.0344 - Time: 0.20s\n",
      "Epoch 536/20000 - Loss: 0.0352 - Time: 0.19s\n",
      "Epoch 537/20000 - Loss: 0.0352 - Time: 0.20s\n",
      "Epoch 538/20000 - Loss: 0.0346 - Time: 0.19s\n",
      "Epoch 539/20000 - Loss: 0.0352 - Time: 0.19s\n",
      "Epoch 540/20000 - Loss: 0.0354 - Time: 0.19s\n",
      "Epoch 541/20000 - Loss: 0.0347 - Time: 0.19s\n",
      "Epoch 542/20000 - Loss: 0.0354 - Time: 0.20s\n",
      "Epoch 543/20000 - Loss: 0.0351 - Time: 0.19s\n",
      "Epoch 544/20000 - Loss: 0.0354 - Time: 0.20s\n",
      "Epoch 545/20000 - Loss: 0.0354 - Time: 0.19s\n",
      "Epoch 546/20000 - Loss: 0.0351 - Time: 0.20s\n",
      "Epoch 547/20000 - Loss: 0.0348 - Time: 0.19s\n",
      "Epoch 548/20000 - Loss: 0.0345 - Time: 0.19s\n",
      "Epoch 549/20000 - Loss: 0.0345 - Time: 0.19s\n",
      "Epoch 550/20000 - Loss: 0.0342 - Time: 0.19s\n",
      "Epoch 551/20000 - Loss: 0.0340 - Time: 0.19s\n",
      "Epoch 552/20000 - Loss: 0.0349 - Time: 0.19s\n",
      "Epoch 553/20000 - Loss: 0.0347 - Time: 0.19s\n",
      "Epoch 554/20000 - Loss: 0.0346 - Time: 0.19s\n",
      "Epoch 555/20000 - Loss: 0.0342 - Time: 0.19s\n",
      "Epoch 556/20000 - Loss: 0.0342 - Time: 0.19s\n",
      "Epoch 557/20000 - Loss: 0.0343 - Time: 0.19s\n",
      "Epoch 558/20000 - Loss: 0.0345 - Time: 0.19s\n",
      "Epoch 559/20000 - Loss: 0.0346 - Time: 0.19s\n",
      "Epoch 560/20000 - Loss: 0.0340 - Time: 0.19s\n",
      "Epoch 561/20000 - Loss: 0.0346 - Time: 0.19s\n",
      "Epoch 562/20000 - Loss: 0.0342 - Time: 0.19s\n",
      "Epoch 563/20000 - Loss: 0.0342 - Time: 0.19s\n",
      "Epoch 564/20000 - Loss: 0.0337 - Time: 0.19s\n",
      "Epoch 565/20000 - Loss: 0.0343 - Time: 0.19s\n",
      "Epoch 566/20000 - Loss: 0.0339 - Time: 0.19s\n",
      "Epoch 567/20000 - Loss: 0.0339 - Time: 0.19s\n",
      "Epoch 568/20000 - Loss: 0.0343 - Time: 0.20s\n",
      "Epoch 569/20000 - Loss: 0.0333 - Time: 0.19s\n",
      "Epoch 570/20000 - Loss: 0.0331 - Time: 0.20s\n",
      "Epoch 571/20000 - Loss: 0.0334 - Time: 0.19s\n",
      "Epoch 572/20000 - Loss: 0.0343 - Time: 0.19s\n",
      "Epoch 573/20000 - Loss: 0.0333 - Time: 0.19s\n",
      "Epoch 574/20000 - Loss: 0.0333 - Time: 0.19s\n",
      "Epoch 575/20000 - Loss: 0.0332 - Time: 0.20s\n",
      "Epoch 576/20000 - Loss: 0.0338 - Time: 0.20s\n",
      "Epoch 577/20000 - Loss: 0.0346 - Time: 0.19s\n",
      "Epoch 578/20000 - Loss: 0.0342 - Time: 0.19s\n",
      "Epoch 579/20000 - Loss: 0.0341 - Time: 0.20s\n",
      "Epoch 580/20000 - Loss: 0.0336 - Time: 0.20s\n",
      "Epoch 581/20000 - Loss: 0.0342 - Time: 0.20s\n",
      "Epoch 582/20000 - Loss: 0.0337 - Time: 0.20s\n",
      "Epoch 583/20000 - Loss: 0.0339 - Time: 0.20s\n",
      "Epoch 584/20000 - Loss: 0.0333 - Time: 0.20s\n",
      "Epoch 585/20000 - Loss: 0.0333 - Time: 0.20s\n",
      "Epoch 586/20000 - Loss: 0.0331 - Time: 0.20s\n",
      "Epoch 587/20000 - Loss: 0.0338 - Time: 0.20s\n",
      "Epoch 588/20000 - Loss: 0.0331 - Time: 0.19s\n",
      "Epoch 589/20000 - Loss: 0.0336 - Time: 0.20s\n",
      "Epoch 590/20000 - Loss: 0.0339 - Time: 0.20s\n",
      "Epoch 591/20000 - Loss: 0.0335 - Time: 0.19s\n",
      "Epoch 592/20000 - Loss: 0.0332 - Time: 0.20s\n",
      "Epoch 593/20000 - Loss: 0.0335 - Time: 0.20s\n",
      "Epoch 594/20000 - Loss: 0.0336 - Time: 0.20s\n",
      "Epoch 595/20000 - Loss: 0.0328 - Time: 0.19s\n",
      "Epoch 596/20000 - Loss: 0.0331 - Time: 0.20s\n",
      "Epoch 597/20000 - Loss: 0.0333 - Time: 0.19s\n",
      "Epoch 598/20000 - Loss: 0.0330 - Time: 0.20s\n",
      "Epoch 599/20000 - Loss: 0.0338 - Time: 0.20s\n",
      "Epoch 600/20000 - Loss: 0.0334 - Time: 0.19s\n",
      "Epoch 601/20000 - Loss: 0.0324 - Time: 0.19s\n",
      "Epoch 602/20000 - Loss: 0.0331 - Time: 0.20s\n",
      "Epoch 603/20000 - Loss: 0.0328 - Time: 0.20s\n",
      "Epoch 604/20000 - Loss: 0.0335 - Time: 0.19s\n",
      "Epoch 605/20000 - Loss: 0.0330 - Time: 0.19s\n",
      "Epoch 606/20000 - Loss: 0.0333 - Time: 0.20s\n",
      "Epoch 607/20000 - Loss: 0.0327 - Time: 0.19s\n",
      "Epoch 608/20000 - Loss: 0.0328 - Time: 0.19s\n",
      "Epoch 609/20000 - Loss: 0.0325 - Time: 0.20s\n",
      "Epoch 610/20000 - Loss: 0.0332 - Time: 0.20s\n",
      "Epoch 611/20000 - Loss: 0.0324 - Time: 0.19s\n",
      "Epoch 612/20000 - Loss: 0.0330 - Time: 0.20s\n",
      "Epoch 613/20000 - Loss: 0.0326 - Time: 0.20s\n",
      "Epoch 614/20000 - Loss: 0.0325 - Time: 0.19s\n",
      "Epoch 615/20000 - Loss: 0.0325 - Time: 0.19s\n",
      "Epoch 616/20000 - Loss: 0.0328 - Time: 0.20s\n",
      "Epoch 617/20000 - Loss: 0.0329 - Time: 0.20s\n",
      "Epoch 618/20000 - Loss: 0.0326 - Time: 0.20s\n",
      "Epoch 619/20000 - Loss: 0.0326 - Time: 0.20s\n",
      "Epoch 620/20000 - Loss: 0.0326 - Time: 0.20s\n",
      "Epoch 621/20000 - Loss: 0.0332 - Time: 0.19s\n",
      "Epoch 622/20000 - Loss: 0.0323 - Time: 0.20s\n",
      "Epoch 623/20000 - Loss: 0.0319 - Time: 0.19s\n",
      "Epoch 624/20000 - Loss: 0.0330 - Time: 0.20s\n",
      "Epoch 625/20000 - Loss: 0.0322 - Time: 0.19s\n",
      "Epoch 626/20000 - Loss: 0.0324 - Time: 0.19s\n",
      "Epoch 627/20000 - Loss: 0.0322 - Time: 0.19s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    t = time.perf_counter()\n",
    "    loss = trainer(unlabeled_images, unlabeled_masks, autoencoder, augment, optimizer, scheduler, scaler, masked_mse)\n",
    "    losses[epoch] = loss.item()\n",
    "    if epoch + 1 in ckpt:\n",
    "        torch.save(autoencoder.state_dict(), f\"{ckpt_path}/{str(config)}/autoencoder_{epoch + 1}.pth\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {loss:.4f} - Time: {time.perf_counter() - t:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
