{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models.efficientnet import MBConvConfig, FusedMBConvConfig\n",
    "\n",
    "import optuna\n",
    "\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling\")\n",
    "from preprocessing import to_NCHW, pad_to_384x384, standardize_images\n",
    "from autoencoder import EfficientNetEncoder, EfficientNetDecoder, AutoencoderConfig\n",
    "from classification import train_and_validate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_amp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data = np.load(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/data/array_data.npz\")\n",
    "unlabeled_images, unlabeled_masks, labeled_images, labeled_masks, labels = data[\"unlabeled_images\"], data[\"unlabeled_masks\"], data[\"labeled_images\"], data[\"labeled_masks\"], data[\"labels\"]\n",
    "\n",
    "unlabeled_images = pad_to_384x384(to_NCHW(unlabeled_images))\n",
    "unlabeled_masks = pad_to_384x384(unlabeled_masks)\n",
    "\n",
    "labeled_images = pad_to_384x384(to_NCHW(labeled_images))\n",
    "labeled_masks = pad_to_384x384(labeled_masks)\n",
    "labels = pad_to_384x384(labels)\n",
    "\n",
    "# Convert to tensors and move to GPU\n",
    "unlabeled_images = torch.tensor(unlabeled_images, dtype=torch.float32).to(device)  # [161, 8, 384, 384]\n",
    "unlabeled_masks = torch.tensor(unlabeled_masks, dtype=torch.bool).to(device)    # [161, 384, 384]\n",
    "\n",
    "labeled_images = torch.tensor(labeled_images, dtype=torch.float32).to(device)      # [3, 8, 384, 384]\n",
    "labeled_masks = torch.tensor(labeled_masks, dtype=torch.bool).to(device)        # [3, 384, 384]\n",
    "labels = torch.tensor(labels, dtype=torch.long).to(device)                      # [3, 384, 384]\n",
    "\n",
    "\n",
    "# Standardize images\n",
    "unlabeled_images, std_channel, mean_channel = standardize_images(unlabeled_images, unlabeled_masks)\n",
    "labeled_images, _, _ = standardize_images(labeled_images, labeled_masks, std_channel, mean_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/ckpt\"\n",
    "saved_epoch = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 15000, 20000, 25000, 30000, 35000, 40000]\n",
    "\n",
    "all_features = torch.zeros((2, 2, 2, 2, 2, len(saved_epoch), 3, 64, 384, 384)).to(device)\n",
    "\n",
    "num_layers_per_block = [1, 2]\n",
    "augementation = [True, False]\n",
    "for block1, block2, block3, flip, rotate in itertools.product(*[num_layers_per_block] * 3,*[augementation] * 2):\n",
    "    config = AutoencoderConfig(\n",
    "        num_layers_block=[block1, block2, block3],\n",
    "        augmentation_flip=flip,\n",
    "        augmentation_rotate=rotate\n",
    "    )\n",
    "    encoder_config = [\n",
    "        FusedMBConvConfig(1, 3, 1, 16, 16, config.num_layers_block[0]),  # 384x384x8 -> 384x384x16\n",
    "        FusedMBConvConfig(4, 3, 2, 16, 32, config.num_layers_block[1]),  # 384x384x16 -> 192x192x32\n",
    "        MBConvConfig(4, 3, 2, 32, 64, config.num_layers_block[2]),       # 192x192x32 -> 96x96x64\n",
    "    ]\n",
    "\n",
    "    encoder = EfficientNetEncoder(\n",
    "        inverted_residual_setting=encoder_config,\n",
    "        dropout=0.1,\n",
    "        input_channels=8,\n",
    "        last_channel=64,\n",
    "    )\n",
    "\n",
    "    decoder = EfficientNetDecoder()\n",
    "    autoencoder = nn.Sequential(encoder, decoder).train().to(device)\n",
    "\n",
    "    folder_path = os.path.join(ckpt_path, str(config))\n",
    "    \n",
    "    for i, epoch in enumerate(saved_epoch):\n",
    "        autoencoder.load_state_dict(torch.load(os.path.join(folder_path, f\"autoencoder_{epoch}.pth\")))\n",
    "        autoencoder.eval()\n",
    "        with torch.inference_mode():\n",
    "            encoder = autoencoder[0]\n",
    "            features = encoder(labeled_images)\n",
    "            features = nn.functional.interpolate(features, size=384, mode=\"bicubic\", antialias=True)\n",
    "            all_features[block1-1, block2-1, block3-1, int(flip), int(rotate), i] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@torch.compile\n",
    "def train_and_validate(\n",
    "    train_data, train_labels, val_data, val_labels,\n",
    "    in_channels, num_layers, kernel_size, hidden_channels,\n",
    "    epochs, lr, weight_decay, optimizer_class, loss_mix_ratio, l1, class_weight, device\n",
    "):\n",
    "    # Create the classifier\n",
    "    #classifier = nn.Conv2d(in_channels, 1, kernel_size=kernel_size, \n",
    "    #                      padding=\"same\", padding_mode=\"replicate\").to(device)\n",
    "    classifier = ConvHead(in_channels, 1, num_layers, kernel_size, hidden_channels).to(device)\n",
    "    classifier.train()\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = optimizer_class(classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred = classifier(train_data)\n",
    "\n",
    "        bce_loss = masked_bce_loss(pred, train_labels, class_weight=class_weight)\n",
    "        hinge_loss = masked_hinge_loss(pred, train_labels, class_weight=class_weight)\n",
    "\n",
    "        loss = loss_mix_ratio * bce_loss + (1 - loss_mix_ratio) * hinge_loss\n",
    "        # Add L1 regularization\n",
    "        loss = loss + l1 * l1_reg(classifier)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    classifier.eval()\n",
    "    with torch.inference_mode():\n",
    "        val_pred = classifier(val_data)\n",
    "        val_loss, val_acc, val_f1 = masked_hinge_loss(val_pred, val_labels, acc=True, f1=True)\n",
    "\n",
    "    return val_f1, val_acc\n",
    "'''\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters with updated API calls\n",
    "    epochs = trial.suggest_int(\"epochs\", 0, 800)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"AdamW\"])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 1, 64)\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 1, 10)\n",
    "    loss_mix_ratio = trial.suggest_float(\"loss_mix_ratio\", 0, 1)\n",
    "    l1 = trial.suggest_float(\"l1\", 1e-5, 5e-1, log=True)\n",
    "\n",
    "    block1 = trial.suggest_int(\"block1\", 1, 2)\n",
    "    block2 = trial.suggest_int(\"block2\", 1, 2)\n",
    "    block3 = trial.suggest_int(\"block3\", 1, 2)\n",
    "    flip = trial.suggest_categorical(\"flip\", [True, False])\n",
    "    rotate = trial.suggest_categorical(\"rotate\", [True, False])\n",
    "    autoencoder_epoch = trial.suggest_int(\"autoencoder_epoch\", 0, len(saved_epoch)-1)\n",
    "\n",
    "    with_orginal = trial.suggest_categorical(\"with_orginal\", [True, False])\n",
    "    class_weight = trial.suggest_categorical(\"class_weight\", [\"balanced\", None])\n",
    "\n",
    "    feature = all_features[block1-1, block2-1, block3-1, int(flip), int(rotate), autoencoder_epoch]\n",
    "    feature, _, _ = standardize_images(feature, labeled_masks)\n",
    "\n",
    "    if with_orginal:\n",
    "        feature = torch.cat([feature, labeled_images], dim=1)\n",
    "    \n",
    "    # Map string to actual optimizer class\n",
    "    optimizer_class = torch.optim.SGD if optimizer_name == \"SGD\" else torch.optim.AdamW\n",
    "\n",
    "    # Cross-validation indices (modify as needed)\n",
    "    train_val_idx = [0, 1]\n",
    "    \n",
    "    # Container for metrics from each fold\n",
    "    fold_records_f1 = torch.zeros(len(train_val_idx))  # For F1 (objective)\n",
    "    fold_records_acc = torch.zeros(len(train_val_idx))  # For accuracy (logging)\n",
    "\n",
    "    # Assuming feature and labels are defined globally (e.g., torch tensors)\n",
    "    for i in train_val_idx:\n",
    "        # Leave-one-out style split\n",
    "        train_idx = [j for j in train_val_idx if j != i]\n",
    "        val_idx = [i]\n",
    "\n",
    "        # Get training and validation data\n",
    "        train_data = feature[train_idx]\n",
    "        train_labels = labels[train_idx]\n",
    "        val_data = feature[val_idx]\n",
    "        val_labels = labels[val_idx]\n",
    "\n",
    "        # Train and validate, get both F1 and accuracy\n",
    "        val_f1, val_acc = train_and_validate(\n",
    "            train_data=train_data,\n",
    "            train_labels=train_labels,\n",
    "            val_data=val_data,\n",
    "            val_labels=val_labels,\n",
    "            in_channels=feature.shape[1],\n",
    "            num_layers=num_layers,\n",
    "            kernel_size=kernel_size,\n",
    "            hidden_channels=hidden_channels,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            optimizer_class=optimizer_class,\n",
    "            loss_mix_ratio=loss_mix_ratio,\n",
    "            l1=l1,\n",
    "            class_weight=class_weight,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        fold_records_f1[i] = val_f1\n",
    "        fold_records_acc[i] = val_acc\n",
    "    \n",
    "    avg_f1 = fold_records_f1.mean().item()\n",
    "    avg_acc = fold_records_acc.mean().item()\n",
    "\n",
    "    trial.set_user_attr(\"val_acc\", avg_acc)\n",
    "    #print(f\"Trial {trial.number}: F1 = {avg_f1}, Acc = {avg_acc}\")\n",
    "    \n",
    "    # Return average F1 score across folds\n",
    "    return avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-20 01:07:05,329] A new study created in memory with name: no-name-f073689a-8d71-43e9-8422-b28d24739df4\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0320 01:07:12.630000 29587 site-packages/torch/_logging/_internal.py:1089] [14/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "[I 2025-03-20 01:07:14,586] Trial 0 finished with value: 0.7487846612930298 and parameters: {'epochs': 19, 'lr': 0.014311267887254743, 'weight_decay': 0.000765175760092528, 'optimizer': 'SGD', 'num_layers': 2, 'hidden_channels': 51, 'kernel_size': 5, 'loss_mix_ratio': 0.7378849884405055, 'l1': 6.860584358007564e-05, 'block1': 1, 'block2': 1, 'block3': 1, 'flip': False, 'rotate': False, 'autoencoder_epoch': 8, 'with_orginal': True, 'class_weight': None}. Best is trial 0 with value: 0.7487846612930298.\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "[I 2025-03-20 01:07:20,805] Trial 1 finished with value: 0.7207898497581482 and parameters: {'epochs': 221, 'lr': 0.00044202195158827575, 'weight_decay': 1.7367361409307996e-05, 'optimizer': 'AdamW', 'num_layers': 2, 'hidden_channels': 12, 'kernel_size': 5, 'loss_mix_ratio': 0.34937514295288996, 'l1': 0.17275412284722694, 'block1': 1, 'block2': 1, 'block3': 2, 'flip': True, 'rotate': False, 'autoencoder_epoch': 13, 'with_orginal': True, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7487846612930298.\n",
      "/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_dynamo/utils.py:2586: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return node.target(*args, **kwargs)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\", \"L['self'].param_groups[0]['params'][6].grad\", \"L['self'].param_groups[0]['params'][7].grad\", \"L['self'].param_groups[0]['params'][8].grad\", \"L['self'].param_groups[0]['params'][9].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "[I 2025-03-20 01:07:26,615] Trial 2 finished with value: 0.682159423828125 and parameters: {'epochs': 65, 'lr': 0.00014415711999832522, 'weight_decay': 0.021854689978301128, 'optimizer': 'SGD', 'num_layers': 3, 'hidden_channels': 44, 'kernel_size': 2, 'loss_mix_ratio': 0.8772783582867998, 'l1': 7.780581252433231e-05, 'block1': 1, 'block2': 1, 'block3': 2, 'flip': True, 'rotate': False, 'autoencoder_epoch': 2, 'with_orginal': False, 'class_weight': None}. Best is trial 0 with value: 0.7487846612930298.\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\", \"L['self'].param_groups[0]['params'][2].grad\", \"L['self'].param_groups[0]['params'][3].grad\", \"L['self'].param_groups[0]['params'][4].grad\", \"L['self'].param_groups[0]['params'][5].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "[I 2025-03-20 01:07:34,955] Trial 3 finished with value: 0.699116051197052 and parameters: {'epochs': 514, 'lr': 0.023416313108571937, 'weight_decay': 0.00046963097340131383, 'optimizer': 'SGD', 'num_layers': 2, 'hidden_channels': 44, 'kernel_size': 10, 'loss_mix_ratio': 0.7898241890967461, 'l1': 0.008108330707335569, 'block1': 2, 'block2': 1, 'block3': 1, 'flip': False, 'rotate': True, 'autoencoder_epoch': 10, 'with_orginal': True, 'class_weight': None}. Best is trial 0 with value: 0.7487846612930298.\n",
      "W0320 01:07:35.183000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [6/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0320 01:07:35.183000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [6/8]    function: 'forward' (/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/classification.py:151)\n",
      "W0320 01:07:35.183000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [6/8]    last reason: 6/0: tensor 'L['self']._modules['layers']._modules['0']._modules['conv']._parameters['weight']' size mismatch at index 0. expected 51, actual 8. Guard failed on a parameter, consider using torch._dynamo.config.force_parameter_static_shapes = False to allow dynamism on parameters.\n",
      "W0320 01:07:35.183000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [6/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0320 01:07:35.183000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "W0320 01:07:35.899000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [14/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0320 01:07:35.899000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [14/8]    function: 'wrapper' (/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/optimizer.py:473)\n",
      "W0320 01:07:35.899000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [14/8]    last reason: 14/7: Cache line invalidated because L['args'][0] got deallocated\n",
      "W0320 01:07:35.899000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [14/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0320 01:07:35.899000 29587 site-packages/torch/_dynamo/convert_frame.py:906] [14/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[I 2025-03-20 01:07:40,390] Trial 4 finished with value: 0.8027840852737427 and parameters: {'epochs': 726, 'lr': 0.0004768793967687634, 'weight_decay': 5.844127878654714e-05, 'optimizer': 'SGD', 'num_layers': 2, 'hidden_channels': 8, 'kernel_size': 8, 'loss_mix_ratio': 0.8055752319841993, 'l1': 0.04591472195151379, 'block1': 2, 'block2': 1, 'block3': 1, 'flip': True, 'rotate': True, 'autoencoder_epoch': 8, 'with_orginal': False, 'class_weight': 'balanced'}. Best is trial 4 with value: 0.8027840852737427.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-20 01:07:46,025] Trial 5 failed with parameters: {'epochs': 579, 'lr': 0.0004428125925564031, 'weight_decay': 0.20653964619466667, 'optimizer': 'AdamW', 'num_layers': 2, 'hidden_channels': 25, 'kernel_size': 10, 'loss_mix_ratio': 0.9475613975557331, 'l1': 0.0001929596201453534, 'block1': 2, 'block2': 1, 'block3': 1, 'flip': False, 'rotate': False, 'autoencoder_epoch': 5, 'with_orginal': False, 'class_weight': None} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_29587/2853001422.py\", line 91, in objective\n",
      "    val_f1, val_acc = train_and_validate(\n",
      "                      ~~~~~~~~~~~~~~~~~~^\n",
      "        train_data=train_data,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<14 lines>...\n",
      "        device=device\n",
      "        ^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/classification.py\", line 154, in train_and_validate\n",
      "    @torch.compile\n",
      "  File \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/classification.py\", line 164, in torch_dynamo_resume_in_train_and_validate_at_164\n",
      "    classifier = ConvHead(in_channels, 1, num_layers, kernel_size, hidden_channels).to(device)\n",
      "  File \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/classification.py\", line 168, in torch_dynamo_resume_in_train_and_validate_at_164\n",
      "    optimizer = optimizer_class(classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
      "  File \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/classification.py\", line 177, in torch_dynamo_resume_in_train_and_validate_at_168\n",
      "    hinge_loss = masked_hinge_loss(pred, train_labels, class_weight=class_weight)\n",
      "  File \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/classification.py\", line 92, in masked_hinge_loss\n",
      "    pred_valid = pred[mask]\n",
      "  File \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/classification.py\", line 93, in torch_dynamo_resume_in_masked_hinge_loss_at_92\n",
      "    label_valid = label[mask]\n",
      "  File \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/classification.py\", line 93, in torch_dynamo_resume_in_masked_hinge_loss_at_93\n",
      "    label_valid = label[mask]\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py\", line 745, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py\", line 1184, in forward\n",
      "    return compiled_fn(full_args)\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 310, in runtime_wrapper\n",
      "    all_outs = call_func_at_runtime_with_args(\n",
      "        compiled_fn, args_, disable_amp=disable_amp, steal_args=True\n",
      "    )\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n",
      "    out = normalize_as_list(f(args))\n",
      "                            ~^^^^^^\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\n",
      "    return f(*args)\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/autograd/function.py\", line 575, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 1585, in forward\n",
      "    fw_outs = call_func_at_runtime_with_args(\n",
      "        CompiledFunction.compiled_fw,\n",
      "        args,\n",
      "        disable_amp=disable_amp,\n",
      "    )\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n",
      "    out = normalize_as_list(f(args))\n",
      "                            ~^^^^^^\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 490, in wrapper\n",
      "    return compiled_fn(runtime_args)\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 672, in inner_fn\n",
      "    outs = compiled_fn(args)\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_inductor/output_code.py\", line 466, in __call__\n",
      "    return self.current_callable(inputs)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_inductor/utils.py\", line 2128, in run\n",
      "    return model(new_inputs)\n",
      "  File \"/tmp/torchinductor_azhang19/wj/cwjf6fo2unsfi5gqabblxkdmqqvhl5y2ivb53clkzybrzng3p4vq.py\", line 212, in call\n",
      "    torch.cuda.set_device(0)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 476, in set_device\n",
      "    torch._C._cuda_setDevice(device)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-20 01:07:46,031] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Optimize the study by running a number of trials (e.g., 100 trials).\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     88\u001b[39m val_labels = labels[val_idx]\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Train and validate, get both F1 and accuracy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m val_f1, val_acc = \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_mix_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_mix_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml1\u001b[49m\u001b[43m=\u001b[49m\u001b[43ml1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m fold_records_f1[i] = val_f1\n\u001b[32m    111\u001b[39m fold_records_acc[i] = val_acc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:574\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m saved_dynamic_layer_stack_depth = (\n\u001b[32m    570\u001b[39m     torch._C._functorch.get_dynamic_layer_stack_depth()\n\u001b[32m    571\u001b[39m )\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    576\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    577\u001b[39m     torch._C._functorch.pop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[32m    578\u001b[39m         saved_dynamic_layer_stack_depth\n\u001b[32m    579\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat 214/stat-214-lab2-group6/code/modeling/classification.py:154\u001b[39m, in \u001b[36mtrain_and_validate\u001b[39m\u001b[34m(train_data, train_labels, val_data, val_labels, in_channels, num_layers, kernel_size, hidden_channels, epochs, lr, weight_decay, optimizer_class, loss_mix_ratio, l1, class_weight, device, return_classifier)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    152\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers(x)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;129m@torch\u001b[39m.compile\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_validate\u001b[39m(\n\u001b[32m    156\u001b[39m     train_data, train_labels, val_data, val_labels,\n\u001b[32m    157\u001b[39m     in_channels, num_layers, kernel_size, hidden_channels,\n\u001b[32m    158\u001b[39m     epochs, lr, weight_decay, optimizer_class, loss_mix_ratio, l1, class_weight,\n\u001b[32m    159\u001b[39m     device, return_classifier=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    160\u001b[39m ):\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# Create the classifier\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m#classifier = nn.Conv2d(in_channels, 1, kernel_size=kernel_size, \u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m#                      padding=\"same\", padding_mode=\"replicate\").to(device)\u001b[39;00m\n\u001b[32m    164\u001b[39m     classifier = ConvHead(in_channels, \u001b[32m1\u001b[39m, num_layers, kernel_size, hidden_channels).to(device)\n\u001b[32m    165\u001b[39m     classifier.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat 214/stat-214-lab2-group6/code/modeling/classification.py:164\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_train_and_validate_at_164\u001b[39m\u001b[34m(___stack0, train_data, train_labels, val_data, val_labels, epochs, lr, weight_decay, optimizer_class, loss_mix_ratio, l1, class_weight, device, return_classifier)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;129m@torch\u001b[39m.compile\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_validate\u001b[39m(\n\u001b[32m    156\u001b[39m     train_data, train_labels, val_data, val_labels,\n\u001b[32m   (...)\u001b[39m\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m#classifier = nn.Conv2d(in_channels, 1, kernel_size=kernel_size, \u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m#                      padding=\"same\", padding_mode=\"replicate\").to(device)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     classifier = ConvHead(in_channels, \u001b[32m1\u001b[39m, num_layers, kernel_size, hidden_channels).to(device)\n\u001b[32m    165\u001b[39m     classifier.train()\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# Instantiate the optimizer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat 214/stat-214-lab2-group6/code/modeling/classification.py:168\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_train_and_validate_at_164\u001b[39m\u001b[34m(___stack0, train_data, train_labels, val_data, val_labels, epochs, lr, weight_decay, optimizer_class, loss_mix_ratio, l1, class_weight, return_classifier)\u001b[39m\n\u001b[32m    165\u001b[39m classifier.train()\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Instantiate the optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m optimizer = optimizer_class(classifier.parameters(), lr=lr, weight_decay=weight_decay)\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat 214/stat-214-lab2-group6/code/modeling/classification.py:177\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_train_and_validate_at_168\u001b[39m\u001b[34m(___stack0, train_data, train_labels, val_data, val_labels, epochs, loss_mix_ratio, l1, class_weight, return_classifier, classifier)\u001b[39m\n\u001b[32m    174\u001b[39m pred = classifier(train_data)\n\u001b[32m    176\u001b[39m bce_loss = masked_bce_loss(pred, train_labels, class_weight=class_weight)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m hinge_loss = \u001b[43mmasked_hinge_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m loss = loss_mix_ratio * bce_loss + (\u001b[32m1\u001b[39m - loss_mix_ratio) * hinge_loss\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# Add L1 regularization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat 214/stat-214-lab2-group6/code/modeling/classification.py:92\u001b[39m, in \u001b[36mmasked_hinge_loss\u001b[39m\u001b[34m(pred, label, acc, f1, class_weight)\u001b[39m\n\u001b[32m     89\u001b[39m label = label.flatten()\n\u001b[32m     90\u001b[39m mask = (label != \u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m pred_valid = pred[mask]\n\u001b[32m     93\u001b[39m label_valid = label[mask]\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Compute class weights if \"balanced\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat 214/stat-214-lab2-group6/code/modeling/classification.py:93\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_masked_hinge_loss_at_92\u001b[39m\u001b[34m(___stack0, label, acc, f1, class_weight, mask)\u001b[39m\n\u001b[32m     90\u001b[39m mask = (label != \u001b[32m0\u001b[39m)\n\u001b[32m     92\u001b[39m pred_valid = pred[mask]\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m label_valid = label[mask]\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Compute class weights if \"balanced\"\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat 214/stat-214-lab2-group6/code/modeling/classification.py:93\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_masked_hinge_loss_at_93\u001b[39m\u001b[34m(___stack0, acc, f1, class_weight, pred_valid)\u001b[39m\n\u001b[32m     90\u001b[39m mask = (label != \u001b[32m0\u001b[39m)\n\u001b[32m     92\u001b[39m pred_valid = pred[mask]\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m label_valid = label[mask]\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Compute class weights if \"balanced\"\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py:1184\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1182\u001b[39m full_args.extend(params_flat)\n\u001b[32m   1183\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:310\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    304\u001b[39m     \u001b[38;5;66;03m# It's possible to have trace_joint inside user specified with no_grad() region,\u001b[39;00m\n\u001b[32m    305\u001b[39m     \u001b[38;5;66;03m# if there is a nested with enable_grad(), that forces some outputs to require gradients.\u001b[39;00m\n\u001b[32m    306\u001b[39m     \u001b[38;5;66;03m# Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.\u001b[39;00m\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd._force_original_view_tracking(\n\u001b[32m    308\u001b[39m         \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    309\u001b[39m     ), torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m         all_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    314\u001b[39m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[32m    315\u001b[39m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[32m    316\u001b[39m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[32m    319\u001b[39m     grad_enabled = torch.is_grad_enabled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[39m, in \u001b[36mmake_boxed_func.<locals>.g\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mg\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1585\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[39m\u001b[34m(ctx, *deduped_flat_tensor_args)\u001b[39m\n\u001b[32m   1576\u001b[39m     ctx._compiled_autograd_backward_state = bw_state\n\u001b[32m   1578\u001b[39m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[32m   1579\u001b[39m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[32m   1580\u001b[39m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1583\u001b[39m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[32m   1584\u001b[39m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1585\u001b[39m fw_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1589\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1591\u001b[39m num_outputs = CompiledFunction.metadata.num_outputs\n\u001b[32m   1592\u001b[39m num_outputs_aliased = CompiledFunction.metadata.num_outputs_aliased\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:490\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    483\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    484\u001b[39m         runtime_metadata,\n\u001b[32m    485\u001b[39m         out,\n\u001b[32m    486\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    487\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    488\u001b[39m     )\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:672\u001b[39m, in \u001b[36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    669\u001b[39m     args = [*([\u001b[38;5;28;01mNone\u001b[39;00m] * num_tokens), *args]\n\u001b[32m    670\u001b[39m     old_args.clear()\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m outs = \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_inductor/output_code.py:466\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    468\u001b[39m     AutotuneCacheBundler.end_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_inductor/utils.py:2128\u001b[39m, in \u001b[36malign_inputs_from_check_idxs.<locals>.run\u001b[39m\u001b[34m(new_inputs)\u001b[39m\n\u001b[32m   2126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(new_inputs: List[InputType]):\n\u001b[32m   2127\u001b[39m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/torchinductor_azhang19/wj/cwjf6fo2unsfi5gqabblxkdmqqvhl5y2ivb53clkzybrzng3p4vq.py:212\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    210\u001b[39m assert_size_stride(primals_3, (s0, ), (\u001b[32m1\u001b[39m, ))\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda._DeviceGuard(\u001b[32m0\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     buf0 = empty_strided_cuda((\u001b[32m9\u001b[39m, ), (\u001b[32m1\u001b[39m, ), torch.float32)\n\u001b[32m    214\u001b[39m     \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [mul, sub, weighted_hinge, loss], Original ATen: [aten.mul, aten.rsub, aten.mean]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/cuda/__init__.py:476\u001b[39m, in \u001b[36mset_device\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    474\u001b[39m device = _get_device_index(device)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device >= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_setDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Optimize the study by running a number of trials (e.g., 100 trials).\n",
    "study.optimize(objective, n_trials=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"optuna_study_autoencoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(study, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization.matplotlib\n",
    "optuna.visualization.matplotlib.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"optuna_study_autoencoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(study, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"optuna_study_autoencoder.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbest_params\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/study.py:119\u001b[39m, in \u001b[36mStudy.best_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbest_params\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    109\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return parameters of the best trial in the study.\u001b[39;00m\n\u001b[32m    110\u001b[39m \n\u001b[32m    111\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbest_trial\u001b[49m.params\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/study.py:162\u001b[39m, in \u001b[36mStudy.best_trial\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_multi_objective():\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    158\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    160\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m best_trial = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_study_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# If the trial with the best value is infeasible, select the best trial from all feasible\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# trials. Note that the behavior is undefined when constrained optimization without the\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# violation value in the best-valued trial.\u001b[39;00m\n\u001b[32m    167\u001b[39m constraints = best_trial.system_attrs.get(_CONSTRAINTS_KEY)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/storages/_in_memory.py:249\u001b[39m, in \u001b[36mInMemoryStorage.get_best_trial\u001b[39m\u001b[34m(self, study_id)\u001b[39m\n\u001b[32m    246\u001b[39m best_trial_id = \u001b[38;5;28mself\u001b[39m._studies[study_id].best_trial_id\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_trial_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo trials are completed yet.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._studies[study_id].directions) > \u001b[32m1\u001b[39m:\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    252\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "test.best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
