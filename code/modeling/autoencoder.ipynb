{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision\n",
    "from torchvision.models.efficientnet import MBConvConfig, FusedMBConvConfig\n",
    "\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling\")\n",
    "from preprocessing import to_NCHW, pad_to_384x384, standardize_images\n",
    "from autoencoder import EfficientNetEncoder, EfficientNetDecoder, AutoencoderConfig, masked_mse\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_amp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data = np.load(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/data/array_data.npz\")\n",
    "unlabeled_images, unlabeled_masks, labeled_images, labeled_masks, labels = data[\"unlabeled_images\"], data[\"unlabeled_masks\"], data[\"labeled_images\"], data[\"labeled_masks\"], data[\"labels\"]\n",
    "\n",
    "unlabeled_images = pad_to_384x384(to_NCHW(unlabeled_images))\n",
    "unlabeled_masks = pad_to_384x384(unlabeled_masks)\n",
    "\n",
    "labeled_images = pad_to_384x384(to_NCHW(labeled_images))\n",
    "labeled_masks = pad_to_384x384(labeled_masks)\n",
    "labels = pad_to_384x384(labels)\n",
    "\n",
    "# Convert to tensors and move to GPU\n",
    "unlabeled_images = torch.tensor(unlabeled_images, dtype=torch.float32).to(device)  # [161, 8, 384, 384]\n",
    "unlabeled_masks = torch.tensor(unlabeled_masks, dtype=torch.bool).to(device)    # [161, 384, 384]\n",
    "\n",
    "labeled_images = torch.tensor(labeled_images, dtype=torch.float32).to(device)      # [3, 8, 384, 384]\n",
    "labeled_masks = torch.tensor(labeled_masks, dtype=torch.bool).to(device)        # [3, 384, 384]\n",
    "labels = torch.tensor(labels, dtype=torch.long).to(device)                      # [3, 384, 384]\n",
    "\n",
    "\n",
    "# Standardize images\n",
    "unlabeled_images, std_channel, mean_channel = standardize_images(unlabeled_images, unlabeled_masks)\n",
    "labeled_images, _, _ = standardize_images(labeled_images, labeled_masks, std_channel, mean_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Configure the Autoencoder model\")\n",
    "\n",
    "    # Argument for num_layers_block (list of integers)\n",
    "    parser.add_argument(\n",
    "        '--num-layers-block',\n",
    "        type=int,\n",
    "        nargs='+',  # Accepts multiple values as a list\n",
    "        default=[1, 1, 1],  # Default value\n",
    "        help=\"Number of layers in each block (e.g., --num-layers-block 1 1 1)\"\n",
    "    )\n",
    "\n",
    "    # Argument for augmentation_flip (boolean)\n",
    "    parser.add_argument(\n",
    "        '--augmentation-flip',\n",
    "        action='store_true',  # Sets to True if flag is present, False otherwise\n",
    "        default=False,  # Default value\n",
    "        help=\"Enable random flip augmentation\"\n",
    "    )\n",
    "\n",
    "    # Argument for augmentation_rotate (boolean)\n",
    "    parser.add_argument(\n",
    "        '--augmentation-rotate',\n",
    "        action='store_true',  # Sets to True if flag is present, False otherwise\n",
    "        default=False,  # Default value\n",
    "        help=\"Enable random rotation augmentation\"\n",
    "    )\n",
    "\n",
    "# Parse arguments\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n",
    "config = AutoencoderConfig(\n",
    "        num_layers_block=args.num_layers_block,\n",
    "        augmentation_flip=args.augmentation_flip,\n",
    "        augmentation_rotate=args.augmentation_rotate\n",
    "    )\n",
    "\n",
    "#config = AutoencoderConfig(num_layers_block=[1, 1, 1], augmentation_flip=True, augmentation_rotate=True)\n",
    "print(config)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoencoderConfig(\n",
    "    num_layers_block=[1, 1, 1],\n",
    "    augmentation_flip=True,\n",
    "    augmentation_rotate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = []\n",
    "if config.augmentation_flip:\n",
    "    augmentation.append(torchvision.transforms.RandomHorizontalFlip(p=0.5))\n",
    "    augmentation.append(torchvision.transforms.RandomVerticalFlip(p=0.5))\n",
    "if config.augmentation_rotate:\n",
    "    augmentation.append(torchvision.transforms.RandomRotation(degrees=180, expand=True,\n",
    "                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR))\n",
    "    augmentation.append(torchvision.transforms.RandomCrop(size=384))\n",
    "augmentation = torchvision.transforms.Compose(augmentation)\n",
    "\n",
    "def apply_augment(images, masks, augmentation):\n",
    "    images_masks = torch.cat([masks.unsqueeze(1).float(), images], dim=1)\n",
    "    images_masks = [augmentation(image_mask) for image_mask in images_masks]\n",
    "    images_masks = torch.stack(images_masks)\n",
    "    return images_masks[:, 1:], images_masks[:, 0] > 0.5\n",
    "\n",
    "augment = lambda images, masks: apply_augment(images, masks, augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = [\n",
    "    FusedMBConvConfig(1, 3, 1, 16, 16, config.num_layers_block[0]),  # 384x384x8 -> 384x384x16\n",
    "    FusedMBConvConfig(4, 3, 2, 16, 32, config.num_layers_block[1]),  # 384x384x16 -> 192x192x32\n",
    "    MBConvConfig(4, 3, 2, 32, 64, config.num_layers_block[2]),       # 192x192x32 -> 96x96x64\n",
    "]\n",
    "\n",
    "# Build encoder and decoder\n",
    "encoder = EfficientNetEncoder(\n",
    "    inverted_residual_setting=encoder_config,\n",
    "    dropout=0.1,\n",
    "    input_channels=8,\n",
    "    last_channel=64,\n",
    ")\n",
    "\n",
    "decoder = EfficientNetDecoder()\n",
    "\n",
    "autoencoder = nn.Sequential(encoder, decoder).train().to(device)\n",
    "#compiled_autoencoder = torch.compile(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in autoencoder: {count_parameters(autoencoder[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40000\n",
    "ckpt = [100, 200, 400, 800, 1600, 3200, 6400, 12800, 15000, 20000, 25000, 30000, 35000, 40000]\n",
    "initial_lr = 1e-3  # Moderate starting LR for AdamW\n",
    "weight_decay = 1e-2  # Regularization for small dataset\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(autoencoder.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)  # Decay to near-zero\n",
    "scaler = torch.amp.GradScaler(device, enabled=use_amp)\n",
    "\n",
    "losses = np.zeros(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def trainer(images, masks, model, augment, optimizer, scheduler, scaler, loss_fn):\n",
    "    with torch.inference_mode():\n",
    "        images, masks = augment(images, masks)\n",
    "    images, masks = images.clone(), masks.clone()\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    with torch.amp.autocast(device, enabled=use_amp):\n",
    "        reconstructions = model(images)\n",
    "        loss = loss_fn(images, masks, reconstructions)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/ckpt\"\n",
    "os.makedirs(f\"{ckpt_path}/{str(config)}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    t = time.perf_counter()\n",
    "    loss = trainer(unlabeled_images, unlabeled_masks, autoencoder, augment, optimizer, scheduler, scaler, masked_mse)\n",
    "    losses[epoch] = loss.item()\n",
    "    if epoch + 1 in ckpt:\n",
    "        torch.save(autoencoder.state_dict(), f\"{ckpt_path}/{str(config)}/autoencoder_{epoch + 1}.pth\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {loss:.4f} - Time: {time.perf_counter() - t:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
