{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.models.efficientnet import MBConvConfig, FusedMBConvConfig\n",
    "\n",
    "import optuna\n",
    "\n",
    "sys.path.append(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling\")\n",
    "from preprocessing import to_NCHW, pad_to_384x384, standardize_images\n",
    "from autoencoder import EfficientNetEncoder, EfficientNetDecoder, AutoencoderConfig\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_amp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data = np.load(\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/data/array_data.npz\")\n",
    "unlabeled_images, unlabeled_masks, labeled_images, labeled_masks, labels = data[\"unlabeled_images\"], data[\"unlabeled_masks\"], data[\"labeled_images\"], data[\"labeled_masks\"], data[\"labels\"]\n",
    "\n",
    "unlabeled_images = pad_to_384x384(to_NCHW(unlabeled_images))\n",
    "unlabeled_masks = pad_to_384x384(unlabeled_masks)\n",
    "\n",
    "labeled_images = pad_to_384x384(to_NCHW(labeled_images))\n",
    "labeled_masks = pad_to_384x384(labeled_masks)\n",
    "labels = pad_to_384x384(labels)\n",
    "\n",
    "# Convert to tensors and move to GPU\n",
    "unlabeled_images = torch.tensor(unlabeled_images, dtype=torch.float32).to(device)  # [161, 8, 384, 384]\n",
    "unlabeled_masks = torch.tensor(unlabeled_masks, dtype=torch.bool).to(device)    # [161, 384, 384]\n",
    "\n",
    "labeled_images = torch.tensor(labeled_images, dtype=torch.float32).to(device)      # [3, 8, 384, 384]\n",
    "labeled_masks = torch.tensor(labeled_masks, dtype=torch.bool).to(device)        # [3, 384, 384]\n",
    "labels = torch.tensor(labels, dtype=torch.long).to(device)                      # [3, 384, 384]\n",
    "\n",
    "\n",
    "# Standardize images\n",
    "unlabeled_images, std_channel, mean_channel = standardize_images(unlabeled_images, unlabeled_masks)\n",
    "labeled_images, _, _ = standardize_images(labeled_images, labeled_masks, std_channel, mean_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = [\n",
    "    FusedMBConvConfig(1, 3, 1, 16, 16, 1),  # 384x384x8 -> 384x384x16\n",
    "    FusedMBConvConfig(4, 3, 2, 16, 32, 1),  # 384x384x16 -> 192x192x32\n",
    "    MBConvConfig(4, 3, 2, 32, 64, 1),       # 192x192x32 -> 96x96x64\n",
    "]\n",
    "\n",
    "# Build encoder and decoder\n",
    "encoder = EfficientNetEncoder(\n",
    "    inverted_residual_setting=encoder_config,\n",
    "    dropout=0.1,\n",
    "    input_channels=8,\n",
    "    last_channel=64,\n",
    ")\n",
    "\n",
    "decoder = EfficientNetDecoder()\n",
    "\n",
    "autoencoder = nn.Sequential(encoder, decoder).train().to(device)\n",
    "autoencoder.load_state_dict(torch.load(f\"/jet/home/azhang19/stat 214/stat-214-lab2-group6/code/modeling/ckpt/AutoencoderConfig([1, 1, 1], flip=True, rotate=True)/autoencoder_12800.pth\"))\n",
    "\n",
    "encoder = encoder.eval()\n",
    "with torch.inference_mode():\n",
    "    feature = encoder(labeled_images)\n",
    "    feature = nn.functional.interpolate(feature, size=384, mode=\"bicubic\", antialias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    train_data, train_labels, val_data, val_labels,\n",
    "    kernel_size, epochs, lr, weight_decay, optimizer_class, loss_fn, l1, device\n",
    "):\n",
    "    # Create the classifier\n",
    "    classifier = nn.Conv2d(64, 1, kernel_size=kernel_size, \n",
    "                          padding=\"same\", padding_mode=\"replicate\").to(device)\n",
    "    classifier.train()\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = optimizer_class(classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred = classifier(train_data)\n",
    "        loss, acc, f1 = loss_fn(pred, train_labels)\n",
    "        # Add L1 regularization\n",
    "        loss = loss + l1 * l1_reg(classifier)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    classifier.eval()\n",
    "    with torch.inference_mode():\n",
    "        val_pred = classifier(val_data)\n",
    "        val_loss, val_acc, val_f1 = loss_fn(val_pred, val_labels)\n",
    "\n",
    "    return val_f1\n",
    "\n",
    "# Apply torch.compile for optimization (if supported)\n",
    "train_and_validate = torch.compile(train_and_validate)\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters with updated API calls\n",
    "    epochs = trial.suggest_int(\"epochs\", 200, 600)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"AdamW\"])\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [1, 2, 3])\n",
    "    loss_name = trial.suggest_categorical(\"loss_fn\", [\"bce\", \"soft_margin\"])\n",
    "    l1 = trial.suggest_float(\"l1\", 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    # Map string to actual optimizer class\n",
    "    optimizer_class = torch.optim.SGD if optimizer_name == \"SGD\" else torch.optim.AdamW\n",
    "\n",
    "    # Map string to loss function (assumes these are defined)\n",
    "    loss_fn = masked_bce_loss_acc if loss_name == \"bce\" else masked_bce_loss_acc\n",
    "\n",
    "    # Cross-validation indices (modify as needed)\n",
    "    train_val_idx = [0, 1]\n",
    "    \n",
    "    # Container for metrics from each fold\n",
    "    fold_records = torch.zeros(len(train_val_idx))\n",
    "\n",
    "    # Assuming feature and labels are defined globally (e.g., torch tensors)\n",
    "    for i in train_val_idx:\n",
    "        # Leave-one-out style split\n",
    "        train_idx = [j for j in train_val_idx if j != i]\n",
    "        val_idx = [i]\n",
    "\n",
    "        # Get training and validation data\n",
    "        train_data = feature[train_idx]\n",
    "        train_labels = labels[train_idx]\n",
    "        val_data = feature[val_idx]\n",
    "        val_labels = labels[val_idx]\n",
    "\n",
    "        # Train and validate\n",
    "        val_f1 = train_and_validate(\n",
    "            train_data=train_data,\n",
    "            train_labels=train_labels,\n",
    "            val_data=val_data,\n",
    "            val_labels=val_labels,\n",
    "            kernel_size=kernel_size,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            optimizer_class=optimizer_class,\n",
    "            loss_fn=loss_fn,\n",
    "            l1=l1,\n",
    "            device=device  # Assumes device is defined globally (e.g., 'cuda')\n",
    "        )\n",
    "\n",
    "        fold_records[i] = val_f1\n",
    "    \n",
    "    # Return average F1 score across folds\n",
    "    return fold_records.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-08 03:49:25,238] A new study created in memory with name: no-name-0dfe4a55-dfa8-45a4-9b6e-913dc081c328\n",
      "W0308 03:49:29.497000 33388 site-packages/torch/_logging/_internal.py:1089] [12/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "[I 2025-03-08 03:49:33,369] Trial 0 finished with value: 0.7514251470565796 and parameters: {'epochs': 241, 'lr': 0.0019239022916231173, 'weight_decay': 0.08630383971405899, 'optimizer': 'AdamW', 'kernel_size': 1, 'loss_fn': 'soft_margin', 'l1': 1.3585939602963995e-05}. Best is trial 0 with value: 0.7514251470565796.\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "[I 2025-03-08 03:49:35,676] Trial 1 finished with value: 0.747683048248291 and parameters: {'epochs': 382, 'lr': 0.0007860536806019399, 'weight_decay': 3.343757860538064e-05, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.009819999518524846}. Best is trial 0 with value: 0.7514251470565796.\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "[I 2025-03-08 03:49:37,680] Trial 2 finished with value: 0.7471671104431152 and parameters: {'epochs': 490, 'lr': 0.002309065656545035, 'weight_decay': 1.866550296324259e-05, 'optimizer': 'AdamW', 'kernel_size': 1, 'loss_fn': 'soft_margin', 'l1': 3.182593537284931e-05}. Best is trial 0 with value: 0.7514251470565796.\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "('Grad tensors [\"L['self'].param_groups[0]['params'][0].grad\", \"L['self'].param_groups[0]['params'][1].grad\"] will be copied during cudagraphs execution.If using cudagraphs and the grad tensor addresses will be the same across runs, use torch._dynamo.decorators.mark_static_address to elide this copy.',)\n",
      "[I 2025-03-08 03:49:39,042] Trial 3 finished with value: 0.7541537284851074 and parameters: {'epochs': 217, 'lr': 0.013749503761034247, 'weight_decay': 0.01042074191799733, 'optimizer': 'SGD', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 8.594022737063027e-05}. Best is trial 3 with value: 0.7541537284851074.\n",
      "W0308 03:49:39.124000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [12/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0308 03:49:39.124000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [12/8]    function: 'wrapper' (/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/optimizer.py:473)\n",
      "W0308 03:49:39.124000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [12/8]    last reason: 12/7: Cache line invalidated because L['args'][0] got deallocated\n",
      "W0308 03:49:39.124000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [12/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0308 03:49:39.124000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [12/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[I 2025-03-08 03:49:40,438] Trial 4 finished with value: 0.7616556286811829 and parameters: {'epochs': 519, 'lr': 0.000989389024146181, 'weight_decay': 0.000582597607128556, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 2.2381831080207123e-05}. Best is trial 4 with value: 0.7616556286811829.\n",
      "[I 2025-03-08 03:49:41,242] Trial 5 finished with value: 0.7595746517181396 and parameters: {'epochs': 323, 'lr': 0.005460131442835135, 'weight_decay': 0.687276381981117, 'optimizer': 'SGD', 'kernel_size': 2, 'loss_fn': 'bce', 'l1': 0.025175166447985075}. Best is trial 4 with value: 0.7616556286811829.\n",
      "[I 2025-03-08 03:49:41,927] Trial 6 finished with value: 0.680161714553833 and parameters: {'epochs': 249, 'lr': 0.0001510624600642446, 'weight_decay': 0.011301301135933065, 'optimizer': 'AdamW', 'kernel_size': 1, 'loss_fn': 'soft_margin', 'l1': 0.08034456674870298}. Best is trial 4 with value: 0.7616556286811829.\n",
      "[I 2025-03-08 03:49:43,097] Trial 7 finished with value: 0.7777446508407593 and parameters: {'epochs': 482, 'lr': 0.000552198761756142, 'weight_decay': 0.26953888148514693, 'optimizer': 'SGD', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.006987588991526098}. Best is trial 7 with value: 0.7777446508407593.\n",
      "W0308 03:49:43.100000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [6/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0308 03:49:43.100000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [6/8]    function: 'torch_dynamo_resume_in_train_and_validate_at_7' (/var/tmp/ipykernel_33388/665618713.py:7)\n",
      "W0308 03:49:43.100000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [6/8]    last reason: 6/0: L['lr'] == 0.0019239022916231173                            \n",
      "W0308 03:49:43.100000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [6/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0308 03:49:43.100000 33388 site-packages/torch/_dynamo/convert_frame.py:906] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[I 2025-03-08 03:49:44,644] Trial 8 finished with value: 0.8042434453964233 and parameters: {'epochs': 586, 'lr': 0.0048757763461574535, 'weight_decay': 0.0009047641753317743, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.024550944038255744}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:45,357] Trial 9 finished with value: 0.7671691179275513 and parameters: {'epochs': 307, 'lr': 0.0845626609948912, 'weight_decay': 0.0002567442357621806, 'optimizer': 'SGD', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.008383036300385699}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:46,882] Trial 10 finished with value: 0.7650431394577026 and parameters: {'epochs': 597, 'lr': 0.021389888939900924, 'weight_decay': 0.0011991182296600923, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.0007199314619076337}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:48,238] Trial 11 finished with value: 0.6804091334342957 and parameters: {'epochs': 597, 'lr': 0.00023434852112346212, 'weight_decay': 0.6870881874420559, 'optimizer': 'SGD', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.0019433059958966308}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:49,367] Trial 12 finished with value: 0.7688026428222656 and parameters: {'epochs': 499, 'lr': 0.00043197785889894876, 'weight_decay': 0.06760736778603929, 'optimizer': 'SGD', 'kernel_size': 2, 'loss_fn': 'bce', 'l1': 0.0017982861013396865}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:50,337] Trial 13 finished with value: 0.7130574584007263 and parameters: {'epochs': 431, 'lr': 0.006470738467559902, 'weight_decay': 0.00014169631220214607, 'optimizer': 'SGD', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.09973287022922503}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:51,741] Trial 14 finished with value: 0.750632643699646 and parameters: {'epochs': 551, 'lr': 0.0006310429136254358, 'weight_decay': 0.004507695899118418, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'soft_margin', 'l1': 0.008616410721166798}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:52,767] Trial 15 finished with value: 0.7566707134246826 and parameters: {'epochs': 439, 'lr': 0.0001003839614847324, 'weight_decay': 0.05703297369325728, 'optimizer': 'SGD', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.0003492285123796947}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:53,990] Trial 16 finished with value: 0.7815059423446655 and parameters: {'epochs': 547, 'lr': 0.028557161580893234, 'weight_decay': 0.0020258292623592136, 'optimizer': 'SGD', 'kernel_size': 2, 'loss_fn': 'bce', 'l1': 0.02589889601277426}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:55,368] Trial 17 finished with value: 0.8006798028945923 and parameters: {'epochs': 550, 'lr': 0.04575904106820774, 'weight_decay': 0.0019693904997047417, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'bce', 'l1': 0.029303414534749828}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:56,799] Trial 18 finished with value: 0.7860784530639648 and parameters: {'epochs': 558, 'lr': 0.084161359712783, 'weight_decay': 8.746045465542729e-05, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.03560353695515594}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:57,737] Trial 19 finished with value: 0.7674981951713562 and parameters: {'epochs': 391, 'lr': 0.03845619563795345, 'weight_decay': 0.00044477622456269323, 'optimizer': 'AdamW', 'kernel_size': 1, 'loss_fn': 'bce', 'l1': 0.003324067396483302}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:49:58,900] Trial 20 finished with value: 0.7742750644683838 and parameters: {'epochs': 451, 'lr': 0.009147665777833887, 'weight_decay': 0.009590989884491419, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.00024002209715563767}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:00,345] Trial 21 finished with value: 0.790658712387085 and parameters: {'epochs': 562, 'lr': 0.06359426411870432, 'weight_decay': 6.735425965597132e-05, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.02963580160642574}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:01,789] Trial 22 finished with value: 0.7925394773483276 and parameters: {'epochs': 570, 'lr': 0.0422615202944554, 'weight_decay': 5.711383419528172e-05, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.04966720888152605}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:03,281] Trial 23 finished with value: 0.7824810147285461 and parameters: {'epochs': 597, 'lr': 0.042833794456172206, 'weight_decay': 1.2077762674512276e-05, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.047703945283689445}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:04,591] Trial 24 finished with value: 0.7887977957725525 and parameters: {'epochs': 520, 'lr': 0.01405218792883535, 'weight_decay': 0.0010109477049382127, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.015464360056767172}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:05,883] Trial 25 finished with value: 0.7673389911651611 and parameters: {'epochs': 525, 'lr': 0.004121882675289262, 'weight_decay': 0.003813562144140905, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'bce', 'l1': 0.0045342790956567625}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:07,081] Trial 26 finished with value: 0.7931022644042969 and parameters: {'epochs': 475, 'lr': 0.012008528709499712, 'weight_decay': 0.0002880722504426017, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.01574882654747054}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:08,283] Trial 27 finished with value: 0.7928721904754639 and parameters: {'epochs': 467, 'lr': 0.01583463444644766, 'weight_decay': 0.00019223074930591362, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.015807739921677593}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:09,288] Trial 28 finished with value: 0.7561101913452148 and parameters: {'epochs': 427, 'lr': 0.00736674164269874, 'weight_decay': 0.0004431425698400663, 'optimizer': 'AdamW', 'kernel_size': 1, 'loss_fn': 'bce', 'l1': 0.0032603070313469086}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:10,585] Trial 29 finished with value: 0.7613153457641602 and parameters: {'epochs': 529, 'lr': 0.0021663318494437567, 'weight_decay': 0.03027389032885971, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'bce', 'l1': 0.015258068701457229}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:11,429] Trial 30 finished with value: 0.7545207738876343 and parameters: {'epochs': 360, 'lr': 0.0013119135844061344, 'weight_decay': 0.001672856813619873, 'optimizer': 'AdamW', 'kernel_size': 1, 'loss_fn': 'bce', 'l1': 0.0011204969299291748}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:12,597] Trial 31 finished with value: 0.7944066524505615 and parameters: {'epochs': 465, 'lr': 0.015743870201735764, 'weight_decay': 0.00019487394032742037, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.01677919437858098}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:13,619] Trial 32 finished with value: 0.7865790724754333 and parameters: {'epochs': 411, 'lr': 0.003345814760110844, 'weight_decay': 0.0003118602970233054, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.01574147770672033}. Best is trial 8 with value: 0.8042434453964233.\n",
      "[I 2025-03-08 03:50:14,819] Trial 33 finished with value: 0.8047637939453125 and parameters: {'epochs': 487, 'lr': 0.009880480343107581, 'weight_decay': 0.0008612331169243355, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.06539420615743693}. Best is trial 33 with value: 0.8047637939453125.\n",
      "[I 2025-03-08 03:50:16,077] Trial 34 finished with value: 0.7895293235778809 and parameters: {'epochs': 507, 'lr': 0.023494666027166506, 'weight_decay': 0.0008122484198683024, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.06507922264898985}. Best is trial 33 with value: 0.8047637939453125.\n",
      "[I 2025-03-08 03:50:17,492] Trial 35 finished with value: 0.8092371225357056 and parameters: {'epochs': 575, 'lr': 0.004957679912503642, 'weight_decay': 0.0024827726945572835, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 0.05310907873256568}. Best is trial 35 with value: 0.8092371225357056.\n",
      "[I 2025-03-08 03:50:18,836] Trial 36 finished with value: 0.7617956399917603 and parameters: {'epochs': 574, 'lr': 0.001635686886865829, 'weight_decay': 0.006083362090560906, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'bce', 'l1': 0.09913387764375997}. Best is trial 35 with value: 0.8092371225357056.\n",
      "[I 2025-03-08 03:50:20,132] Trial 37 finished with value: 0.7729146480560303 and parameters: {'epochs': 539, 'lr': 0.0047408501710588975, 'weight_decay': 0.0032190468821673085, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'soft_margin', 'l1': 1.094055608313593e-05}. Best is trial 35 with value: 0.8092371225357056.\n",
      "[I 2025-03-08 03:50:21,402] Trial 38 finished with value: 0.7221376895904541 and parameters: {'epochs': 578, 'lr': 0.0024452356699727505, 'weight_decay': 0.0022636989189098204, 'optimizer': 'AdamW', 'kernel_size': 1, 'loss_fn': 'bce', 'l1': 0.06024661638496883}. Best is trial 35 with value: 0.8092371225357056.\n",
      "[I 2025-03-08 03:50:22,599] Trial 39 finished with value: 0.7748255133628845 and parameters: {'epochs': 498, 'lr': 0.009432903429374471, 'weight_decay': 0.021359753497756977, 'optimizer': 'AdamW', 'kernel_size': 3, 'loss_fn': 'bce', 'l1': 6.725534193977838e-05}. Best is trial 35 with value: 0.8092371225357056.\n",
      "[I 2025-03-08 03:50:23,930] Trial 40 finished with value: 0.8138102889060974 and parameters: {'epochs': 576, 'lr': 0.003024530162365274, 'weight_decay': 0.0007553346861698726, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.03973047964089309}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:25,269] Trial 41 finished with value: 0.8064203262329102 and parameters: {'epochs': 581, 'lr': 0.003183843238759477, 'weight_decay': 0.0008796749478699672, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.037094263847982256}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:26,637] Trial 42 finished with value: 0.8127140998840332 and parameters: {'epochs': 591, 'lr': 0.0037382964865954943, 'weight_decay': 0.0009038011831704703, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.04202858207779916}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:27,983] Trial 43 finished with value: 0.7644068002700806 and parameters: {'epochs': 579, 'lr': 0.0010021385307279155, 'weight_decay': 0.0006390286249140601, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.04335996066842848}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:29,243] Trial 44 finished with value: 0.802624523639679 and parameters: {'epochs': 536, 'lr': 0.0033579568063167803, 'weight_decay': 0.006636301967846832, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.07683858937771006}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:30,635] Trial 45 finished with value: 0.8005142211914062 and parameters: {'epochs': 595, 'lr': 0.0016952225747922458, 'weight_decay': 0.0005461011934349896, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.046786376668371765}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:31,256] Trial 46 finished with value: 0.7507368922233582 and parameters: {'epochs': 265, 'lr': 0.002616199620725168, 'weight_decay': 0.0013048462902799022, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.005337862493142676}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:32,112] Trial 47 finished with value: 0.766848087310791 and parameters: {'epochs': 368, 'lr': 0.006466201065506214, 'weight_decay': 0.00012608041355052739, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.00961169392700233}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:33,289] Trial 48 finished with value: 0.7644574642181396 and parameters: {'epochs': 565, 'lr': 0.004005710173270949, 'weight_decay': 2.700110305744233e-05, 'optimizer': 'SGD', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.02089079052581886}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:34,478] Trial 49 finished with value: 0.809105396270752 and parameters: {'epochs': 510, 'lr': 0.008885358925197139, 'weight_decay': 0.0008249837240904066, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.07308742144836308}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:35,680] Trial 50 finished with value: 0.811099648475647 and parameters: {'epochs': 506, 'lr': 0.006094441378759994, 'weight_decay': 0.015380911597015857, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.035160840815082334}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:36,934] Trial 51 finished with value: 0.8106082677841187 and parameters: {'epochs': 513, 'lr': 0.006021948315365546, 'weight_decay': 0.018247180132671584, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.03564181474638501}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:38,177] Trial 52 finished with value: 0.7831264734268188 and parameters: {'epochs': 506, 'lr': 0.0059800784298627415, 'weight_decay': 0.12412483348345847, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.09861578735800175}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:39,504] Trial 53 finished with value: 0.8013383150100708 and parameters: {'epochs': 545, 'lr': 0.007713344758243456, 'weight_decay': 0.017155635852989962, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.02398910424658654}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:40,658] Trial 54 finished with value: 0.7737671136856079 and parameters: {'epochs': 523, 'lr': 0.005448559879284538, 'weight_decay': 0.04289060110896088, 'optimizer': 'SGD', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.01115954291050743}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:41,162] Trial 55 finished with value: 0.7784665822982788 and parameters: {'epochs': 202, 'lr': 0.004598209493619087, 'weight_decay': 0.010542319889552837, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.03507875223828169}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:42,415] Trial 56 finished with value: 0.8085422515869141 and parameters: {'epochs': 511, 'lr': 0.011210944154271247, 'weight_decay': 0.13273385950623318, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.05542944667943524}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:43,765] Trial 57 finished with value: 0.8021860122680664 and parameters: {'epochs': 554, 'lr': 0.01913553038447027, 'weight_decay': 0.0026059799125262187, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.030207931424792552}. Best is trial 40 with value: 0.8138102889060974.\n",
      "[I 2025-03-08 03:50:44,747] Trial 58 finished with value: 0.7625160813331604 and parameters: {'epochs': 452, 'lr': 0.007883436397400297, 'weight_decay': 0.005335778957531843, 'optimizer': 'SGD', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 2.4154444099140386e-05}. Best is trial 40 with value: 0.8138102889060974.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-08 03:50:45,720] Trial 59 failed with parameters: {'epochs': 599, 'lr': 0.0026581942837536543, 'weight_decay': 0.016468585255105837, 'optimizer': 'AdamW', 'kernel_size': 2, 'loss_fn': 'soft_margin', 'l1': 0.011467467047446327} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/tmp/ipykernel_33388/665618713.py\", line 70, in objective\n",
      "    val_f1 = train_and_validate(\n",
      "        train_data=train_data,\n",
      "    ...<10 lines>...\n",
      "        device=device  # Assumes device is defined globally (e.g., 'cuda')\n",
      "    )\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_33388/665618713.py\", line 6, in train_and_validate\n",
      "    classifier = nn.Conv2d(64, 1, kernel_size=kernel_size,\n",
      "  File \"/var/tmp/ipykernel_33388/665618713.py\", line 6, in torch_dynamo_resume_in_train_and_validate_at_6\n",
      "    classifier = nn.Conv2d(64, 1, kernel_size=kernel_size,\n",
      "  File \"/var/tmp/ipykernel_33388/665618713.py\", line 22, in torch_dynamo_resume_in_train_and_validate_at_7\n",
      "    optimizer.step()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/optimizer.py\", line 493, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/adamw.py\", line 243, in step\n",
      "    adamw(\n",
      "    ~~~~~^\n",
      "        params_with_grad,\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "    ...<18 lines>...\n",
      "        has_complex=has_complex,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/optimizer.py\", line 154, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/adamw.py\", line 875, in adamw\n",
      "    func(\n",
      "    ~~~~^\n",
      "        params,\n",
      "        ^^^^^^^\n",
      "    ...<16 lines>...\n",
      "        has_complex=has_complex,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/jet/home/azhang19/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/adamw.py\", line 702, in _multi_tensor_adamw\n",
      "    torch._foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-08 03:50:45,724] Trial 59 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Optimize the study by running a number of trials (e.g., 100 trials).\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     67\u001b[39m     val_labels = labels[val_idx]\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Train and validate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     val_f1 = \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1\u001b[49m\u001b[43m=\u001b[49m\u001b[43ml1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Assumes device is defined globally (e.g., 'cuda')\u001b[39;49;00m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     fold_records[i] = val_f1\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Return average F1 score across folds\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:574\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m saved_dynamic_layer_stack_depth = (\n\u001b[32m    570\u001b[39m     torch._C._functorch.get_dynamic_layer_stack_depth()\n\u001b[32m    571\u001b[39m )\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    576\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    577\u001b[39m     torch._C._functorch.pop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[32m    578\u001b[39m         saved_dynamic_layer_stack_depth\n\u001b[32m    579\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_and_validate\u001b[39m\u001b[34m(train_data, train_labels, val_data, val_labels, kernel_size, epochs, lr, weight_decay, optimizer_class, loss_fn, l1, device)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_validate\u001b[39m(\n\u001b[32m      2\u001b[39m     train_data, train_labels, val_data, val_labels,\n\u001b[32m      3\u001b[39m     kernel_size, epochs, lr, weight_decay, optimizer_class, loss_fn, l1, device\n\u001b[32m      4\u001b[39m ):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Create the classifier\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     classifier = nn.Conv2d(\u001b[32m64\u001b[39m, \u001b[32m1\u001b[39m, kernel_size=kernel_size, \n\u001b[32m      7\u001b[39m                           padding=\u001b[33m\"\u001b[39m\u001b[33msame\u001b[39m\u001b[33m\"\u001b[39m, padding_mode=\u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m      8\u001b[39m     classifier.train()\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Instantiate the optimizer\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_train_and_validate_at_6\u001b[39m\u001b[34m(___stack0, train_data, train_labels, val_data, val_labels, epochs, lr, weight_decay, optimizer_class, loss_fn, l1, device)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_validate\u001b[39m(\n\u001b[32m      2\u001b[39m     train_data, train_labels, val_data, val_labels,\n\u001b[32m      3\u001b[39m     kernel_size, epochs, lr, weight_decay, optimizer_class, loss_fn, l1, device\n\u001b[32m      4\u001b[39m ):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Create the classifier\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     classifier = nn.Conv2d(\u001b[32m64\u001b[39m, \u001b[32m1\u001b[39m, kernel_size=kernel_size, \n\u001b[32m      7\u001b[39m                           padding=\u001b[33m\"\u001b[39m\u001b[33msame\u001b[39m\u001b[33m\"\u001b[39m, padding_mode=\u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m      8\u001b[39m     classifier.train()\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Instantiate the optimizer\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_train_and_validate_at_7\u001b[39m\u001b[34m(___stack0, train_data, train_labels, val_data, val_labels, epochs, lr, weight_decay, optimizer_class, loss_fn, l1)\u001b[39m\n\u001b[32m     20\u001b[39m     loss = loss + l1 * l1_reg(classifier)\n\u001b[32m     21\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m     25\u001b[39m classifier.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/adamw.py:243\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    230\u001b[39m     beta1, beta2 = cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    232\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    233\u001b[39m         group,\n\u001b[32m    234\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m         state_steps,\n\u001b[32m    241\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/adamw.py:875\u001b[39m, in \u001b[36madamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    873\u001b[39m     func = _single_tensor_adamw\n\u001b[32m--> \u001b[39m\u001b[32m875\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/env_214/lib/python3.13/site-packages/torch/optim/adamw.py:702\u001b[39m, in \u001b[36m_multi_tensor_adamw\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[39m\n\u001b[32m    699\u001b[39m     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n\u001b[32m    701\u001b[39m torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg_sq_sqrt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    703\u001b[39m torch._foreach_addcdiv_(\n\u001b[32m    704\u001b[39m     device_params,\n\u001b[32m    705\u001b[39m     device_exp_avgs,\n\u001b[32m    706\u001b[39m     exp_avg_sq_sqrt,\n\u001b[32m    707\u001b[39m     step_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    708\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Optimize the study by running a number of trials (e.g., 100 trials).\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Best Validation F1 Score: 0.8262\n",
      "  Hyperparameters:\n",
      "    epochs: 325\n",
      "    lr: 0.0003618543621266028\n",
      "    weight_decay: 1.979201053287094e-05\n",
      "    optimizer: SGD\n",
      "    kernel_size: 3\n",
      "    loss_fn: bce\n",
      "    l1: 0.0047670779365748895\n"
     ]
    }
   ],
   "source": [
    "# Print out the best trial.\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  Best Validation F1 Score: {:.4f}\".format(best_trial.value))\n",
    "print(\"  Hyperparameters:\")\n",
    "for param_name, param_value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(param_name, param_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
